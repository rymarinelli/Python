{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rymarinelli/Python/blob/master/Kvasir_Data_Poison_Radiology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwNNVZLqc-WK"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-fTqXAVPIoQ"
      },
      "source": [
        "## Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pQP_-fKc82m",
        "outputId": "a9c0ac96-eb60-434e-fd01-d14495c10111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-24 18:04:01--  https://datasets.simula.no/downloads/kvasir-seg.zip\n",
            "Resolving datasets.simula.no (datasets.simula.no)... 128.39.36.14\n",
            "Connecting to datasets.simula.no (datasets.simula.no)|128.39.36.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46227172 (44M) [application/zip]\n",
            "Saving to: ‘kvasir-seg.zip.2’\n",
            "\n",
            "kvasir-seg.zip.2    100%[===================>]  44.08M  13.1MB/s    in 3.4s    \n",
            "\n",
            "2024-10-24 18:04:05 (13.1 MB/s) - ‘kvasir-seg.zip.2’ saved [46227172/46227172]\n",
            "\n",
            "Archive:  kvasir-seg.zip\n",
            "replace ./kvasir/Kvasir-SEG/kavsir_bboxes.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "! wget https://datasets.simula.no/downloads/kvasir-seg.zip\n",
        "! unzip kvasir-seg.zip -d ./kvasir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk-2WiXDpds_",
        "outputId": "805a28f2-f463-421c-c4cc-1d526d82ea2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training and validation sets created in 'dataset'\n",
            "Training images: 800, Validation images: 200\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def split_dataset(image_dir, mask_dir, output_dir, bbox_json_path, val_split=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Split the dataset into training and validation directories, using only images that have bounding box annotations.\n",
        "\n",
        "    Parameters:\n",
        "        image_dir (str): Path to the images directory.\n",
        "        mask_dir (str): Path to the masks directory.\n",
        "        output_dir (str): Path to the output directory where train/val directories will be created.\n",
        "        bbox_json_path (str): Path to the JSON file containing bounding box annotations.\n",
        "        val_split (float): Fraction of the dataset to use as validation (e.g., 0.2 means 20% validation).\n",
        "        seed (int): Random seed for reproducibility.\n",
        "    \"\"\"\n",
        "    # Load bounding box data from JSON file\n",
        "    with open(bbox_json_path, 'r') as f:\n",
        "        bbox_data = json.load(f)\n",
        "\n",
        "    # Filter images that have bounding box annotations\n",
        "    annotated_images = set(bbox_data.keys())\n",
        "\n",
        "    # training and validation sets\n",
        "    train_image_dir = os.path.join(output_dir, 'train', 'images')\n",
        "    val_image_dir = os.path.join(output_dir, 'val', 'images')\n",
        "    train_mask_dir = os.path.join(output_dir, 'train', 'masks')\n",
        "    val_mask_dir = os.path.join(output_dir, 'val', 'masks')\n",
        "\n",
        "    os.makedirs(train_image_dir, exist_ok=True)\n",
        "    os.makedirs(val_image_dir, exist_ok=True)\n",
        "    os.makedirs(train_mask_dir, exist_ok=True)\n",
        "    os.makedirs(val_mask_dir, exist_ok=True)\n",
        "\n",
        "    #corresponding bounding box annotations\n",
        "    image_filenames = [f for f in os.listdir(image_dir) if f.split('.')[0] in annotated_images]\n",
        "\n",
        "    # Shuffle image filenames\n",
        "    random.seed(seed)\n",
        "    random.shuffle(image_filenames)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    val_size = int(len(image_filenames) * val_split)\n",
        "    val_filenames = image_filenames[:val_size]\n",
        "    train_filenames = image_filenames[val_size:]\n",
        "\n",
        "    # create masks based on bounding boxes\n",
        "    def copy_files(file_list, src_image_dir, dest_image_dir, dest_mask_dir, bbox_data):\n",
        "        for filename in file_list:\n",
        "            # Copy image\n",
        "            shutil.copy(os.path.join(src_image_dir, filename), os.path.join(dest_image_dir, filename))\n",
        "\n",
        "            # Create a corresponding mask based on bounding box data\n",
        "            image_id = filename.split('.')[0]\n",
        "            bbox_info = bbox_data.get(image_id, {})\n",
        "            if 'bbox' in bbox_info:\n",
        "                # Load the image to get its dimensions\n",
        "                image_path = os.path.join(src_image_dir, filename)\n",
        "                image = Image.open(image_path)\n",
        "                width, height = image.size\n",
        "\n",
        "                # Create an empty mask\n",
        "                mask = np.zeros((height, width), dtype=np.uint8)\n",
        "\n",
        "\n",
        "                for bbox in bbox_info['bbox']:\n",
        "                    xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']\n",
        "                    mask[ymin:ymax, xmin:xmax] = 255\n",
        "\n",
        "                # Save the mask\n",
        "                mask = Image.fromarray(mask)\n",
        "                mask_filename = filename.replace('.jpg', '.png')  # Adjust extension if needed\n",
        "                mask.save(os.path.join(dest_mask_dir, mask_filename))\n",
        "            else:\n",
        "                print(f\"Warning: No bounding box found for image '{filename}'\")\n",
        "\n",
        "    # Copy training files and create masks\n",
        "    copy_files(train_filenames, image_dir, train_image_dir, train_mask_dir, bbox_data)\n",
        "\n",
        "    # Copy validation files and create masks\n",
        "    copy_files(val_filenames, image_dir, val_image_dir, val_mask_dir, bbox_data)\n",
        "\n",
        "    print(f\"Training and validation sets created in '{output_dir}'\")\n",
        "    print(f\"Training images: {len(train_filenames)}, Validation images: {len(val_filenames)}\")\n",
        "\n",
        "\n",
        "image_dir = '/content/kvasir/Kvasir-SEG/images'\n",
        "mask_dir = '/content/kvasir/Kvasir-SEG/masks'\n",
        "output_dir = 'dataset'\n",
        "bbox_json_path = '/content/kvasir/Kvasir-SEG/kavsir_bboxes.json'\n",
        "\n",
        "# Split dataset with 20% validation data\n",
        "split_dataset(image_dir, mask_dir, output_dir, bbox_json_path, val_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ADWBZzYne9K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Custom Dataset class for U-Net\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.images[idx]\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "        mask_name = image_name.replace('.jpg', '.png')  # Assuming mask files are .png\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "        # Load image and mask\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')  # 'L' mode for grayscale (single channel)\n",
        "\n",
        "        # Apply transformations if provided\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        # Convert mask to tensor (required for training)\n",
        "        mask = torch.tensor(np.array(mask), dtype=torch.long)  # Ensuring mask is long (integer) type for cross entropy\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj37HsvqniFU"
      },
      "outputs": [],
      "source": [
        "# transformations for both images and masks\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize images and masks to 256x256\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = SegmentationDataset(image_dir='/content/dataset/train',\n",
        "                                    mask_dir='/content/dataset/val',\n",
        "                                    transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE-AqE_UnzXp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Contracting path (Encoder)\n",
        "        self.enc1 = self.conv_block(in_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Expansive path (Decoder)\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.dec4 = self.conv_block(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec3 = self.conv_block(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec2 = self.conv_block(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec1 = self.conv_block(128, 64)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        \"\"\"Two convolutional layers with ReLU and Batch Normalization\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder (Downsampling)\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(nn.MaxPool2d(kernel_size=2, stride=2)(enc1))\n",
        "        enc3 = self.enc3(nn.MaxPool2d(kernel_size=2, stride=2)(enc2))\n",
        "        enc4 = self.enc4(nn.MaxPool2d(kernel_size=2, stride=2)(enc3))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(nn.MaxPool2d(kernel_size=2, stride=2)(enc4))\n",
        "\n",
        "        # Decoder (Upsampling)\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((enc4, dec4), dim=1)  # Skip connection\n",
        "        dec4 = self.dec4(dec4)\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((enc3, dec3), dim=1)  # Skip connection\n",
        "        dec3 = self.dec3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((enc2, dec2), dim=1)  # Skip connection\n",
        "        dec2 = self.dec2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((enc1, dec1), dim=1)  # Skip connection\n",
        "        dec1 = self.dec1(dec1)\n",
        "\n",
        "        # Output\n",
        "        return self.output_conv(dec1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "FyGsV21ADjbj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uovfpacuiO15",
        "outputId": "3d77c720-4bc1-45d7-de29-600266ec3858"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 100/100 [00:11<00:00,  8.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.5102599602937699\n",
            "Validation Loss: 0.4936222195625305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 100/100 [00:11<00:00,  8.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Loss: 0.47347333312034606\n",
            "Validation Loss: 0.4693891930580139\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3/10: 100%|██████████| 100/100 [00:11<00:00,  8.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Loss: 0.45264406204223634\n",
            "Validation Loss: 0.4470020651817322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4/10: 100%|██████████| 100/100 [00:11<00:00,  8.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Loss: 0.4388850110769272\n",
            "Validation Loss: 0.4252496516704559\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5/10: 100%|██████████| 100/100 [00:11<00:00,  8.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/10], Loss: 0.42921296566724776\n",
            "Validation Loss: 0.4480717831850052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6/10: 100%|██████████| 100/100 [00:11<00:00,  8.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/10], Loss: 0.42517987012863157\n",
            "Validation Loss: 0.4377950584888458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7/10: 100%|██████████| 100/100 [00:11<00:00,  8.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/10], Loss: 0.42168295800685884\n",
            "Validation Loss: 0.5769519257545471\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8/10: 100%|██████████| 100/100 [00:11<00:00,  8.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/10], Loss: 0.4175533476471901\n",
            "Validation Loss: 0.40580100417137144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9/10: 100%|██████████| 100/100 [00:11<00:00,  8.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/10], Loss: 0.4058820441365242\n",
            "Validation Loss: 0.40192769646644594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10/10: 100%|██████████| 100/100 [00:11<00:00,  8.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/10], Loss: 0.3984456717967987\n",
            "Validation Loss: 0.39607759833335876\n",
            "Mean Average Precision (mAP) score: 0.4253216268380061\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# Dataset class for loading images and masks\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None, mask_transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        # Filter images to ensure corresponding mask exists\n",
        "        self.images = []\n",
        "        for img_name in os.listdir(image_dir):\n",
        "            mask_name = img_name.replace('.jpg', '.png')\n",
        "            mask_path = os.path.join(mask_dir, mask_name)\n",
        "            if os.path.exists(mask_path):\n",
        "                self.images.append(img_name)\n",
        "            else:\n",
        "                print(f\"Warning: Mask not found for {img_name}, expected path: {mask_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.images[idx]\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "        mask_name = image_name.replace('.jpg', '.png')\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')  # 'L' mode for grayscale (single channel)\n",
        "\n",
        "        # Apply transformations to image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Apply transformations to mask\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        # Convert mask to tensor (0 for background, 1 for the class)\n",
        "        mask = torch.tensor(np.array(mask), dtype=torch.float32) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Define mask transformation (resize only, no need to normalize)\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256), interpolation=Image.NEAREST),  # Use NEAREST for masks to avoid interpolation artifacts\n",
        "])\n",
        "\n",
        "# Initialize dataset and data loader\n",
        "train_dataset = SegmentationDataset(image_dir='/content/dataset/train/images',\n",
        "                                    mask_dir='/content/dataset/train/masks',\n",
        "                                    transform=transform,\n",
        "                                    mask_transform=mask_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "val_dataset = SegmentationDataset(image_dir='/content/dataset/val/images',\n",
        "                                  mask_dir='/content/dataset/val/masks',\n",
        "                                  transform=transform,\n",
        "                                  mask_transform=mask_transform)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize U-Net model, loss function, and optimizer\n",
        "model = UNet(in_channels=3, out_channels=1)  # Change out_channels to 1 for one class (foreground vs background)\n",
        "model = model.to(device)  # Move model to GPU if available\n",
        "\n",
        "# Loss function: Binary Cross Entropy with Logits for one-class segmentation\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "        images = images.to(device)\n",
        "        masks = masks.unsqueeze(1).to(device)  # Add channel dimension for masks\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_images, val_masks in val_loader:\n",
        "            val_images = val_images.to(device)\n",
        "            val_masks = val_masks.unsqueeze(1).to(device)  # Add channel dimension for masks\n",
        "\n",
        "            # Forward pass\n",
        "            val_outputs = model(val_images)\n",
        "\n",
        "            # Calculate loss\n",
        "            val_loss += criterion(val_outputs, val_masks).item()\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss/len(val_loader)}\")\n",
        "\n",
        "# Calculate mAP on validation set\n",
        "def calculate_mAP(model, dataloader, device, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device).view(-1)  # Flatten the masks\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            preds = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
        "            preds = (preds > threshold).float().view(-1)  # Threshold to get binary predictions\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(masks.cpu().numpy())\n",
        "\n",
        "    # Calculate mean Average Precision (mAP)\n",
        "    mAP = average_precision_score(all_labels, all_preds)\n",
        "    return mAP\n",
        "\n",
        "# Calculate mAP for validation set\n",
        "mAP_score = calculate_mAP(model, val_loader, device)\n",
        "print(f\"Mean Average Precision (mAP) score: {mAP_score}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FGSM"
      ],
      "metadata": {
        "id": "wU5vfIBpDoTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoJhWhmriwR9",
        "outputId": "325006ad-e70f-454e-d280-5fe423d02d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision (mAP) score with FGSM adversarial data: 0.22305134803804721\n",
            "FGSM mAP testing complete!\n"
          ]
        }
      ],
      "source": [
        "# FGSM attack to generate adversarial examples\n",
        "def fgsm_attack(model, images, labels, epsilon):\n",
        "    # Set requires_grad attribute of tensor so we can calculate gradient with respect to it\n",
        "    images.requires_grad = True\n",
        "\n",
        "    # Forward pass the data through the model\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Zero all existing gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Collect the element-wise sign of the data gradient\n",
        "    data_grad = images.grad.data.sign()\n",
        "\n",
        "    # Create the perturbed image by adjusting each pixel of the input image\n",
        "    perturbed_images = images + epsilon * data_grad\n",
        "\n",
        "    # Clipping the values to maintain image integrity\n",
        "    perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
        "\n",
        "    # Detach perturbed images to avoid further tracking of gradients\n",
        "    perturbed_images = perturbed_images.detach()\n",
        "    return perturbed_images\n",
        "\n",
        "\n",
        "# Calculate mAP on adversarial (FGSM) validation data\n",
        "def calculate_mAP_with_adversarial(model, dataloader, device, epsilon, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, masks in dataloader:\n",
        "        # Move data to device\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device).unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "        # Generate adversarial examples using FGSM\n",
        "        adv_images = fgsm_attack(model, images, masks, epsilon)\n",
        "\n",
        "        # Forward pass on adversarial images\n",
        "        with torch.no_grad():\n",
        "            outputs = model(adv_images)\n",
        "            preds = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
        "            preds = (preds > threshold).float().view(-1)  # Threshold to get binary predictions\n",
        "\n",
        "            masks = masks.view(-1)  # Flatten the masks\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(masks.cpu().numpy())\n",
        "\n",
        "    # Calculate mean Average Precision (mAP)\n",
        "    mAP = average_precision_score(all_labels, all_preds)\n",
        "    return mAP\n",
        "\n",
        "# Set epsilon for FGSM attack\n",
        "epsilon = 8/255\n",
        "\n",
        "# Calculate mAP for FGSM poisoned validation data\n",
        "mAP_score_adv = calculate_mAP_with_adversarial(model, val_loader, device, epsilon)\n",
        "print(f\"Mean Average Precision (mAP) score with FGSM adversarial data: {mAP_score_adv}\")\n",
        "\n",
        "print(\"FGSM mAP testing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CW"
      ],
      "metadata": {
        "id": "vF8-U1BADvpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "batC5Ujhs1Us",
        "outputId": "a3f71c76-4758-417f-d141-4b71f954aa3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.60it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision (mAP) score with C&W adversarial data: 0.19864990846426359\n",
            "C&W mAP testing complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import average_precision_score\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "\n",
        "# C&W attack to generate adversarial examples for one-class segmentation task\n",
        "def cw_attack(model, images, labels, c=1e-4, kappa=0, num_iter=100, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Performs the Carlini & Wagner (C&W) attack on the input images for one-class segmentation.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.Module): The model to attack.\n",
        "        images (torch.Tensor): The input images.\n",
        "        labels (torch.Tensor): The true labels for the input images (one-class segmentation masks).\n",
        "        c (float): Regularization parameter controlling the importance of misclassification.\n",
        "        kappa (float): Confidence level for making the adversarial example misclassified.\n",
        "        num_iter (int): Number of iterations to optimize the adversarial perturbation.\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The perturbed images.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Ensure images and labels have correct data types\n",
        "    images = images.float()\n",
        "    labels = labels.float()\n",
        "\n",
        "    # Initialize the perturbation (w) with zeros and set requires_grad=True\n",
        "    w = torch.zeros_like(images, requires_grad=True, device=images.device)\n",
        "\n",
        "    # Optimizer for the perturbation\n",
        "    optimizer = optim.Adam([w], lr=learning_rate)\n",
        "\n",
        "    # generate adversarial examples\n",
        "    for _ in tqdm(range(num_iter), desc=\"Generating Adversarial Examples (C&W)\", leave=True):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate the adversarial examples using tanh to ensure values are in [0, 1]\n",
        "        perturbed_images = 0.5 * (torch.tanh(w) + 1)\n",
        "\n",
        "\n",
        "        outputs = model(perturbed_images)  # Outputs should be of shape [batch_size, 1, height, width]\n",
        "\n",
        "        # Apply sigmoid to get probabilities in range [0, 1]\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "\n",
        "        # Misclassification loss for one-class segmentation\n",
        "        # Invert the labels to create the misclassification objective\n",
        "        inverted_labels = 1 - labels  # Treat target class pixels as background and vice versa\n",
        "        f_loss = F.binary_cross_entropy(outputs, inverted_labels)\n",
        "\n",
        "        # Calculate L2 distance per image in batch\n",
        "        l2_dist = torch.sum((perturbed_images - images) ** 2, dim=(1, 2, 3))  # L2 distance for each sample in batch\n",
        "\n",
        "        # Combine the losses element-wise and sum across batch to get the final loss\n",
        "        total_loss = torch.sum(c * f_loss + l2_dist)\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Return final adversarial examples\n",
        "    return perturbed_images.detach()\n",
        "\n",
        "# Calculate mAP on adversarial (C&W) validation data for one-class segmentation\n",
        "def calculate_mAP_with_adversarial_cw(model, dataloader, device, c=1e-4, kappa=0, num_iter=100, learning_rate=0.01, threshold=0.5):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device).unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "        #  progress bar\n",
        "        adv_images = cw_attack(model, images, masks, c=c, kappa=kappa, num_iter=num_iter, learning_rate=learning_rate)\n",
        "\n",
        "        # Forward pass on adversarial images\n",
        "        with torch.no_grad():\n",
        "            outputs = model(adv_images)\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            preds = (preds > threshold).float().view(-1)\n",
        "\n",
        "            masks = masks.view(-1)  # Flatten the masks\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(masks.cpu().numpy())\n",
        "\n",
        "    # Calculate mean Average Precision (mAP)\n",
        "    mAP = average_precision_score(all_labels, all_preds)\n",
        "    return mAP\n",
        "\n",
        "# parameters for C&W attack\n",
        "c = 100       # Increase regularization parameter to emphasize misclassification\n",
        "kappa = 15     # Increase kappa to push the adversarial example confidently away from the correct class\n",
        "num_iter = 100 # Increase number of iterations to ensure better convergence\n",
        "learning_rate = 0.1  # Increase learning rate to optimize perturbations\n",
        "\n",
        "# Calculate mAP for C&W poisoned validation data\n",
        "mAP_score_adv_cw = calculate_mAP_with_adversarial_cw(model, val_loader, device, c=c, kappa=kappa, num_iter=num_iter, learning_rate=learning_rate)\n",
        "print(f\"Mean Average Precision (mAP) score with C&W adversarial data: {mAP_score_adv_cw}\")\n",
        "\n",
        "print(\"C&W mAP testing complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(model, images, labels, epsilon, save_path=None):\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device).unsqueeze(1)  # Add channel dimension to labels to match output shape [batch_size, 1, H, W]\n",
        "\n",
        "    images.requires_grad = True\n",
        "\n",
        "    # Forward pass to get model predictions\n",
        "    outputs = model(images)\n",
        "\n",
        "\n",
        "    print(f\"Output shape: {outputs.shape}, Label shape (after unsqueeze): {labels.shape}\")\n",
        "\n",
        "    # Calculate the loss\n",
        "    if labels.shape != outputs.shape:\n",
        "        raise ValueError(f\"Shape mismatch: Output shape {outputs.shape} and label shape {labels.shape} must be the same.\")\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Get the gradient of the images\n",
        "    data_grad = images.grad.data.sign()\n",
        "\n",
        "    # Create perturbed image by adjusting the input image with epsilon in the direction of the gradient\n",
        "    perturbed_images = images + epsilon * data_grad\n",
        "\n",
        "    # Clip the perturbed images to maintain the range [0, 1]\n",
        "    perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
        "\n",
        "    # Detach the perturbed images from the computation graph\n",
        "    perturbed_images = perturbed_images.detach()\n",
        "\n",
        "\n",
        "    if save_path is not None:\n",
        "        save_image(perturbed_images[0], save_path)\n",
        "        print(f\"Adversarial image saved at: {save_path}\")\n",
        "\n",
        "    return perturbed_images\n",
        "\n",
        "\n",
        "\n",
        "# Get a batch of data from the DataLoader\n",
        "data_iter = iter(val_loader)\n",
        "images, masks = next(data_iter)\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "epsilon = 8 / 255\n",
        "fgsm_save_path = \"./fgsm_adv_image.png\"\n",
        "\n",
        "fgsm_adv_images = fgsm_attack(model, images, masks, epsilon, save_path=fgsm_save_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2IkWUjW21XY",
        "outputId": "37f07807-fbac-4c96-894a-0d5634e3ee7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after unsqueeze): torch.Size([8, 1, 256, 256])\n",
            "Adversarial image saved at: ./fgsm_adv_image.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C&W attack to generate a single adversarial example\n",
        "def cw_attack_single(model, image, label, c=1e-4, kappa=0, num_iter=100, learning_rate=0.01, save_path=None):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Move data to the same device\n",
        "    image = image.to(device).float()\n",
        "    label = label.to(device).float().unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions to label\n",
        "\n",
        "    # Initialize the perturbation (w) with zeros and set requires_grad=True\n",
        "    w = torch.zeros_like(image, requires_grad=True, device=device)\n",
        "\n",
        "    # Optimizer for the perturbation\n",
        "    optimizer = optim.Adam([w], lr=learning_rate)\n",
        "\n",
        "    for _ in tqdm(range(num_iter), desc=\"Generating Adversarial Example (C&W)\", leave=True):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate the adversarial example using tanh to ensure values are in [0, 1]\n",
        "        perturbed_image = 0.5 * (torch.tanh(w) + 1)\n",
        "\n",
        "\n",
        "        output = model(perturbed_image)\n",
        "\n",
        "\n",
        "        output = torch.sigmoid(output)\n",
        "\n",
        "        # Misclassification loss for one-class segmentation\n",
        "        inverted_label = 1 - label  # Treat target class pixels as background and vice versa\n",
        "        f_loss = torch.nn.functional.binary_cross_entropy(output, inverted_label)\n",
        "\n",
        "        # Calculate L2 distance\n",
        "        l2_dist = torch.sum((perturbed_image - image) ** 2)\n",
        "\n",
        "        # Combine the losses\n",
        "        total_loss = c * f_loss + l2_dist\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Detach the perturbed image from the computation graph\n",
        "    perturbed_image = perturbed_image.detach()\n",
        "\n",
        "\n",
        "    if save_path is not None:\n",
        "        save_image(perturbed_image[0], save_path)\n",
        "        print(f\"Adversarial C&W image saved at: {save_path}\")\n",
        "\n",
        "    return perturbed_image\n"
      ],
      "metadata": {
        "id": "ZCKqhfl17JWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and save a single C&W adversarial example\n",
        "data_iter = iter(val_loader)\n",
        "images, masks = next(data_iter)\n",
        "image = images[0].unsqueeze(0)  # Add batch dimension to make it [1, channels, height, width]\n",
        "mask = masks[0]  # Shape: [height, width]\n",
        "\n",
        "c = 100\n",
        "kappa = 15\n",
        "num_iter = 100\n",
        "learning_rate = 0.01\n",
        "cw_adv_image = cw_attack_single(model, image, mask, c=c, kappa=kappa, num_iter=num_iter, learning_rate=learning_rate, save_path=cw_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6YcL3Kz7Omd",
        "outputId": "487b361f-18c8-4bd0-aa37-9aa8bafd702a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Adversarial Example (C&W): 100%|██████████| 100/100 [00:01<00:00, 85.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adversarial C&W image saved at: ./cw_adv_image.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowledge Distillation"
      ],
      "metadata": {
        "id": "M41e19ob8QVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Knowledge distillation parameters\n",
        "alpha = 0.5  # Weight for the ground truth loss (between 0 and 1)\n",
        "temperature = 3.0  # Temperature for softening the logits\n",
        "\n",
        "# Optimizer for the student model\n",
        "optimizer_student = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "\n",
        "# loss functions\n",
        "criterion_ce = nn.BCEWithLogitsLoss()  # Cross-entropy loss for ground truth\n",
        "criterion_kl = nn.KLDivLoss(reduction='batchmean')  # KL divergence loss for distillation\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "        images = images.to(device)\n",
        "        masks = masks.unsqueeze(1).to(device)  # Add channel dimension for masks\n",
        "\n",
        "        # Forward pass through teacher model\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(images)\n",
        "\n",
        "        # Forward pass through student model\n",
        "        student_outputs = student_model(images)\n",
        "\n",
        "        # Distillation loss (KL divergence between teacher and student outputs)\n",
        "        # Apply softmax and log_softmax to the logits after dividing by temperature\n",
        "        teacher_outputs_soft = nn.functional.softmax(teacher_outputs / temperature, dim=1)\n",
        "        student_outputs_soft = nn.functional.log_softmax(student_outputs / temperature, dim=1)\n",
        "\n",
        "        distillation_loss = criterion_kl(student_outputs_soft, teacher_outputs_soft) * (temperature ** 2)\n",
        "\n",
        "        # Ground truth loss (cross-entropy)\n",
        "        ground_truth_loss = criterion_ce(student_outputs, masks)\n",
        "\n",
        "        # Combined loss\n",
        "        loss = alpha * ground_truth_loss + (1 - alpha) * distillation_loss\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer_student.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_student.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "print(\"Knowledge distillation training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz7JUR2S8S9O",
        "outputId": "1fa51674-fda4-46ad-ff70-6a36c3305b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 100/100 [00:08<00:00, 12.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.24849708795547484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 100/100 [00:08<00:00, 12.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.24761902615427972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/10: 100%|██████████| 100/100 [00:08<00:00, 12.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.24735896408557892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/10: 100%|██████████| 100/100 [00:07<00:00, 12.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.2479127061367035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/10: 100%|██████████| 100/100 [00:08<00:00, 12.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.24707938805222512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6/10: 100%|██████████| 100/100 [00:08<00:00, 12.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.24747191920876502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7/10: 100%|██████████| 100/100 [00:07<00:00, 12.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.24667733833193778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8/10: 100%|██████████| 100/100 [00:07<00:00, 12.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.24707041323184967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9/10: 100%|██████████| 100/100 [00:08<00:00, 12.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.24652438178658487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10/10: 100%|██████████| 100/100 [00:07<00:00, 12.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.24702798217535019\n",
            "Knowledge distillation training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mAP for FGSM poisoned validation data\n",
        "# FGSM attack to generate adversarial examples\n",
        "def fgsm_attack(model, images, labels, epsilon, save_path=None):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Move data to the same device\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device).unsqueeze(1)  # Add channel dimension to labels to match output shape [batch_size, 1, H, W]\n",
        "\n",
        "    # Remove extra dimension from labels if present\n",
        "    if labels.dim() == 5 and labels.shape[2] == 1:\n",
        "        labels = labels.squeeze(2)\n",
        "\n",
        "    images.requires_grad = True\n",
        "\n",
        "    # Forward pass to get model predictions\n",
        "    outputs = model(images)\n",
        "\n",
        "\n",
        "    print(f\"Output shape: {outputs.shape}, Label shape (after adjustment): {labels.shape}\")\n",
        "\n",
        "    if labels.shape != outputs.shape:\n",
        "        raise ValueError(f\"Shape mismatch: Output shape {outputs.shape} and label shape {labels.shape} must be the same.\")\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Zero all existing gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass to compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Get the gradient of the images\n",
        "    data_grad = images.grad.data.sign()\n",
        "\n",
        "    # Create perturbed image by adjusting the input image with epsilon in the direction of the gradient\n",
        "    perturbed_images = images + epsilon * data_grad\n",
        "\n",
        "    # Clip the perturbed images to maintain the range [0, 1]\n",
        "    perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
        "\n",
        "    # Detach the perturbed images from the computation graph\n",
        "    perturbed_images = perturbed_images.detach()\n",
        "\n",
        "\n",
        "    if save_path is not None:\n",
        "        save_image(perturbed_images[0], save_path)\n",
        "        print(f\"Adversarial image saved at: {save_path}\")\n",
        "\n",
        "    return perturbed_images\n",
        "\n",
        "epsilon = 8 / 255\n",
        "mAP_score_adv = calculate_mAP_with_adversarial(student_model, val_loader, device, epsilon)\n",
        "print(f\"Mean Average Precision (mAP) score with FGSM adversarial data: {mAP_score_adv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCjg_dAH-LuW",
        "outputId": "c98be1e0-8770-49fe-99e7-43c03343b4cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Output shape: torch.Size([8, 1, 256, 256]), Label shape (after adjustment): torch.Size([8, 1, 256, 256])\n",
            "Mean Average Precision (mAP) score with FGSM adversarial data: 0.22214199474631363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C&W attack to generate adversarial examples for one-class segmentation task\n",
        "def cw_attack(model, images, labels, c=1e-4, kappa=0, num_iter=100, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Performs the Carlini & Wagner (C&W) attack on the input images for one-class segmentation.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.Module): The model to attack.\n",
        "        images (torch.Tensor): The input images.\n",
        "        labels (torch.Tensor): The true labels for the input images (one-class segmentation masks).\n",
        "        c (float): Regularization parameter controlling the importance of misclassification.\n",
        "        kappa (float): Confidence level for making the adversarial example misclassified.\n",
        "        num_iter (int): Number of iterations to optimize the adversarial perturbation.\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The perturbed images.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Ensure images and labels have correct data types\n",
        "    images = images.to(device).float()\n",
        "    labels = labels.to(device).float()  # Ensure labels have correct type and device\n",
        "\n",
        "    # Initialize the perturbation (w) with zeros and set requires_grad=True\n",
        "    w = torch.zeros_like(images, requires_grad=True, device=images.device)\n",
        "\n",
        "    # Optimizer for the perturbation\n",
        "    optimizer = optim.Adam([w], lr=learning_rate)\n",
        "\n",
        "    for _ in tqdm(range(num_iter), desc=\"Generating Adversarial Examples (C&W)\", leave=True):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Calculate the adversarial examples using tanh to ensure values are in [0, 1]\n",
        "        perturbed_images = 0.5 * (torch.tanh(w) + 1)\n",
        "\n",
        "\n",
        "        outputs = model(perturbed_images)  # Outputs should be of shape [batch_size, 1, height, width]\n",
        "\n",
        "\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "\n",
        "\n",
        "        if labels.dim() == 5:\n",
        "            labels = labels.squeeze(2)  # Remove unnecessary dimension if present\n",
        "\n",
        "        # Misclassification loss for one-class segmentation\n",
        "        # Invert the labels to create the misclassification objective\n",
        "        inverted_labels = 1 - labels\n",
        "        f_loss = F.binary_cross_entropy(outputs, inverted_labels)\n",
        "\n",
        "        # Calculate L2 distance per image in batch\n",
        "        l2_dist = torch.sum((perturbed_images - images) ** 2, dim=(1, 2, 3))\n",
        "\n",
        "        # Combine the losses element-wise and sum across batch to get the final loss\n",
        "        total_loss = torch.sum(c * f_loss + l2_dist)\n",
        "\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Detach the perturbed images from the computation graph\n",
        "    perturbed_images = perturbed_images.detach()\n",
        "\n",
        "    return perturbed_images\n",
        "\n",
        "# Calculate mAP for C&W poisoned validation data using the distilled model\n",
        "mAP_score_adv_cw = calculate_mAP_with_adversarial_cw(student_model, val_loader, device, c=c, kappa=kappa, num_iter=num_iter, learning_rate=learning_rate)\n",
        "print(f\"Mean Average Precision (mAP) score with C&W adversarial data for distilled model: {mAP_score_adv_cw}\")\n",
        "\n",
        "print(\"C&W mAP testing for distilled model complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FdcDu0xAdmA",
        "outputId": "0d85fb44-ab04-4852-c7b9-5bc3f9fd5cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 219.02it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.20it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.34it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.41it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.78it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.45it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.35it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.59it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.61it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.31it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.30it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.62it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.61it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.28it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.37it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.60it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.41it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.26it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.72it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.75it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.61it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.36it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.33it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.76it/s]\n",
            "Generating Adversarial Examples (C&W): 100%|██████████| 100/100 [00:00<00:00, 231.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision (mAP) score with C&W adversarial data for distilled model: 0.224517900948618\n",
            "C&W mAP testing for distilled model complete!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPVCIgU0AvsC7p8BKLqu2sK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}