{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rymarinelli/Python/blob/master/DAT945_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e43b4b85",
      "metadata": {
        "id": "e43b4b85"
      },
      "source": [
        "# Assignment 1: Adversarial Examples and Uncertainty in AI Models\n",
        "## DAT945: Secure and Robust AI Model Development\n",
        "\n",
        "### Task 1\n",
        "\n",
        "Consider an image classification model $f(x)$ that takes an input image $x$ and predicts its class label. An adversarial attack aims to generate a modified image $x'$ that is visually similar to the original image $x$ but is misclassified by the model.\n",
        "\n",
        "1. Define the $L_p$-norm and the $L_p$-ball in $n$-dimensional space. Explain how they are utilized in adversarial attacks.\n",
        "\n",
        "In Warr, the $L_p$ is defined by $ \\parallel \\mathbf d\\parallel_p = (| d|_1^p + | d|_2^p + \\ldotp \\ldotp \\ldotp | d|_n^p)^{\\frac{1}{p}}$.\n",
        "These measures to determine how perceivable perturbations are to an image. When p is 1, it is simply the absolute value of the pixel changes. When p is 2, it is euclidean distance.\n",
        "\n",
        "$L_p$-ball constrains the amount of the perbutations by a particular radius in the input space. In essence, the $L_p$-ball should use the distance as defined by $L_p$ to segment.\n",
        "\n",
        "\\begin{equation*}\n",
        "L_pball = \\left\\{ \\mathbf{d} \\in \\mathbb{R}^n : \\left( \\sum_{i=1}^{n} |d_i|^p \\right)^{1/p} < r \\right\\}\n",
        "\\end{equation*}\n",
        "\n",
        "You might also use $L_p$-ball as a means to stress test models. It would give you means of understanding that even if input is shifted by $L_p$ amount, it should still give the correct classification.\n",
        "\n",
        "2. Discuss the importance of the parameter $\\epsilon$ in adversarial attacks. How does adjusting its value influence the effectiveness of the attack?\n",
        "\n",
        "$\\epsilon$ is a parameter that determines the scale of the perbutations. If an $\\epsilon$ is large, then there is a higher chance of detection but a higher chance the model will perform worse. Likewise, if $\\epsilon$ is smaller, it will create smaller perbutations which may be less noticeable but less signficant to the model.\n",
        "\n",
        "3. Contrast targeted and untargeted adversarial attacks. Provide examples of each and describe their respective objectives.\n",
        "\n",
        "Untargeted attacks are broad attacks that seek to generally diminish model performance through adding perbutations to inputs. Targeted attacks are more specific. They may use patches and only manipulate salient pixels. An attacker could trick a model by classifiying wrongly through this salient pixel manipulation.\n",
        "\n",
        "4. Describe the Fast Gradient Sign Method (FGSM) and its process for creating adversarial examples.\n",
        "\n",
        "FGSM calcuates the direction of the input space at the location of image and attempts to approixmate the contours of prediction landscape to misclassify. It does this by manipulating each value to see if the missclassifcation becomes more or less likely. When the direction is determined, it applies pertubations in the pixels to the direction of the desired change. The goal is that through adding these pertubations, the image will be on the precipice of the classifcation space and will be wrongly classified.\n",
        "\n",
        "5. Examine the correlation between the size of perturbation and the success rate of an adversarial attack. How does this interplay inform the concept of adversarial robustness?\n",
        "\n",
        "The larger perturbation, the larger the change is there in the input space and predictions. This will make the misclassifcation more likely. If the bands for prediction are more fine-tuned through considering adversarial examples, the models will be more robust.  \n",
        "\n",
        "6. Apart from the FGSM, list other prevalent methods for crafting adversarial examples. Briefly discuss one of these alternatives.\n",
        "\n",
        "JSMA selects pixels with the largest impact and changes them by a determined amount in the direction to misclassify. This based on using saliency.\n",
        "\n",
        "7. Identify potential real-world applications for adversarial attacks. Suggest defensive measures that could be adopted to mitigate these attacks.\n",
        "\n",
        "One application is to ensure models are more robust. One problem that is well documented is that models had issues detecting people of color. By adding adverisal examples, the models could be evaluated to target these weakences.  \n",
        "\n",
        "---\n",
        "\n",
        "### Task 2\n",
        "\n",
        "Imagine we are using a natural language processing (NLP) model to classify text inputs. Consider the following scenario:\n",
        "\n",
        "We have a dataset consisting of text inputs and their corresponding labels. We want to evaluate the robustness of our NLP model against adversarial text examples created using TextAttack.\n",
        "\n",
        "1. Explain what TextAttack is and how it applies to NLP models.\n",
        "\n",
        "In Morris et al, they describe that TextAttack is their new framework in Python.There are four components: a goal function, a set of constraints, a transformion, and a search method. The goal function determines if an attack is succesful. Constraints determine if the perturbation is valid. Transformation creates a set of potential pertubations, and the search method iteratively generates and selects the the perturbation that most aligns with the the goal.\n",
        "These perturbations can be used to make more robust NLP models. This is done through interchanging synonyms.\n",
        "\n",
        "2. Generate two examples using TextAttack on an NLP model: one with a targeted attack and another with an untargeted attack. Describe the modifications made to the original texts.\n",
        "\n",
        "Using synonymns was the primary tactic to the originial texts. There are two classes in the example below. There are positive and negative review. The targetted attack was conducted on the positive reviews to trick the model into a negatively classifying.\n",
        "\n",
        "Example Results:\n",
        "\n",
        "a high-spirited buddy movie about the reunion of <strong>berlin anarchists</strong> who face arrest 15 years after their crime .\n",
        "\n",
        "a high-spirited buddy cine about the pooling of <strong>germania antifascist</strong> who tussle stops 15 anno after their contravention .\n",
        "\n",
        "\n",
        "\n",
        "3. Discuss the potential impact of adversarial text examples on NLP model performance and decision-making processes.\n",
        "\n",
        "Adversarial examples would make models more robust as they would learn a greater vocabulary and structure of sentences.It may also improve the UI of models. For instance, non-native speakers tend to follow formulaic structures in their sentences.\n",
        "\n",
        "4. Propose strategies for defending NLP models against adversarial text examples. Consider both model hardening and input sanitization approaches.\n",
        "\n",
        "In terms of model hardening, the easiest method might be to train your model on adversial examples. If the model has already learned to deal with the most problematic input, then the input should not be a problem when being run. As for sanitization, tokenizing and lemmatization could be used to reduce the variation in the input. These techniques are rather standard and are likely used as part of NLP processes. Additional techniques of semantic comparison could also be used as an additional filter.\n",
        "\n",
        "---\n",
        "\n",
        "### Task 3\n",
        "\n",
        "Explore the concepts of homomorphic encryption and its application in secure computing:\n",
        "\n",
        "1. Define homomorphic encryption and distinguish it from conventional encryption methods.\n",
        "Homomorphic encryption is a technique that allows for encrypted data to be used for purposes but not be decrypted for use. Other techniques require decryption.\n",
        "\n",
        "2. Identify and explain the two primary forms of homomorphic encryption, noting their differences.\n",
        "There is partial and fully homomorphic. With partial, additional or multiplication can be performed on the encryped data. Full homomorphic allows for a greater amount of operations on the encrypted data.\n",
        "\n",
        "\n",
        "3. How does homomorphic encryption allow computations on encrypted data without disclosing the plaintext? Provide a detailed example.\n",
        "\n",
        "Homomorphic encryption is based on that when computations are performed on encrypted data it will match the plain text computations. One scheme that is used is the Pailler Cryptosystem. This works when adding.\n",
        "\n",
        "Intitally, you would want to determine a key parameter from the multiplication of two prime numbers. You then calculate the private key based on the LCM of the key parameter. Then, you select your public key with the restriction that it is a multiple of the product of the primes selected. You then encrypt based on keys. Through the mathemical properties of factors and primes, the operations should match between the plain text and encrypted.\n",
        "\n",
        "4. Discuss the security attributes of homomorphic encryption, including confidentiality, integrity, and authenticity.\n",
        "\n",
        "In terms of encryption, only the intial party that encrypted the data should be able to unencrypt the data. This should make the data more confidential and promote the authenticity   \n",
        "\n",
        "5. How can homomorphic encryption enhance privacy and security in machine learning applications? Focus on aspects such as predictive modeling and neural network training.\n",
        "\n",
        "Since most of what training in neural networks is just linear algerbra, you should be able train models largely on encrypted data. It may also support efforts in federated learning, through sharing encrypted information.\n",
        "\n",
        "6. Outline the limitations or challenges associated with homomorphic encryption and propose potential solutions.\n",
        "\n",
        "Firstly, it seems computationally intensive to encrypt every piece of data. It might also make it difficult to perform optimization techniques on the data. For instance, I doubt you could quantize the encrypted data. One potential solution would be to look into sharding operations more across greater resources to see if you could avoid differences in inference speed.\n",
        "\n",
        "7. Describe the mechanism of fully homomorphic encryption (FHE) and how it differs from other schemes.\n",
        "\n",
        "FHE uses lattice-based cryptography.It uses vectors as the basis for determining a key, and it is not based in factorizing as was the case in the Pallier cryptosystem which is partially homomorphic.\n",
        "\n",
        "8. Present a practical application of homomorphic encryption in areas like secure cloud computing or privacy-preserving database queries.\n",
        "\n",
        "Since one is able to perform calculations with the encrypted data, it allows for the secure transmission of data from your local workstation to the cloud. Also, I can see some of the pratical approaches to using in a database. Since you can add data securely, all the aggregative queries in database would be correct and secure.\n",
        "\n",
        "9. What are current research challenges in homomorphic encryption, and how might they be addressed?\n",
        "\n",
        "There is a lot of focus how quantum computing will play a role in breaking encryption. The approach is to lattice-based techniques. Previous factorization based techniques will no longer be secure, so developing secure encryption techniques through the development of these newer techniques will be needed.\n",
        "\n",
        "---\n",
        "\n",
        "### Task 4\n",
        "\n",
        "Consider a deep neural network $f_{\\theta}$ for predictive analytics:\n",
        "\n",
        "1. Define and differentiate between aleatoric and epistemic uncertainty in the context of machine learning.\n",
        "\n",
        "Aleatoric Uncertainty is the irreducible uncertainty. It comes from the data itself. Epistemic Uncertainly comes from a lack of understanding about your model and how it is using your data. This can mininized by improving your model by looking at your residuals and attempting to test your model to see where it is failing.\n",
        "\n",
        "2. Suppose the output $y$ of the network is modeled as a Gaussian distribution $p(y|x, \\theta)$. Explain how this model represents aleatoric uncertainty.\n",
        "\n",
        "By modeling the output as a distribution, the model is returning a distribution instead of a point estimate. This representation will be more demonstrative of variation and will highlight greater uncertainty as the estimated variance differs.\n",
        "\n",
        "3. Explore methods for quantifying epistemic uncertainty in deep learning, such as Bayesian approaches. Detail how these methods assess the uncertainty in model parameters.\n",
        "\n",
        "Bayesian Neural Networks are one method. They use a distribution over the weights. By observing the posterior distributions of the weights, one can measure of the uncertainy assioicated with the parameters by measuring the dispersion of the respective distributions.\n",
        "\n",
        "4. Develop a loss function that integrates both aleatoric and epistemic uncertainties, and describe its optimization using stochastic gradient descent.\n",
        "\n",
        "For aleatoric loss, you can use the negative log-likeihood. This provides a probabilistic intrepretation. For epistemic, you can observe the variance of the means. By adding these two losses together, you can estimate combined loss. With each iteration with SGD, the loss get minimized.  \n",
        "\n",
        "5. Discuss the challenges in implementing uncertainty quantification methods in deep learning and propose solutions to mitigate these issues, such as reducing computational demands.\n",
        "\n",
        "Since you are dealing with distributions, it is computionally intensive. One solutions would be to take samples via a Monte Carlo method or another sampling technique.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textattack[tensorflow]\n",
        "!pip install phe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPEI4mT3AqI-",
        "outputId": "55b1f0e9-d8ff-4d0f-a1b2-19dfea84b566"
      },
      "id": "aPEI4mT3AqI-",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textattack[tensorflow]\n",
            "  Downloading textattack-0.3.10-py3-none-any.whl (445 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.7/445.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bert-score>=0.3.5 (from textattack[tensorflow])\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (0.6.2)\n",
            "Collecting flair (from textattack[tensorflow])\n",
            "  Downloading flair-0.13.1-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (3.14.0)\n",
            "Collecting language-tool-python (from textattack[tensorflow])\n",
            "  Downloading language_tool_python-2.8-py3-none-any.whl (35 kB)\n",
            "Collecting lemminflect (from textattack[tensorflow])\n",
            "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lru-dict (from textattack[tensorflow])\n",
            "  Downloading lru_dict-1.3.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting datasets>=2.4.0 (from textattack[tensorflow])\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (2.0.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (1.11.4)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (4.41.2)\n",
            "Collecting terminaltables (from textattack[tensorflow])\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (4.66.4)\n",
            "Collecting word2number (from textattack[tensorflow])\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting num2words (from textattack[tensorflow])\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (10.1.0)\n",
            "Collecting pinyin>=0.4.0 (from textattack[tensorflow])\n",
            "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (0.42.1)\n",
            "Collecting OpenHowNet (from textattack[tensorflow])\n",
            "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tensorflow>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (0.16.1)\n",
            "Collecting tensorflow-text>=2.9.0 (from textattack[tensorflow])\n",
            "  Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX (from textattack[tensorflow])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from textattack[tensorflow]) (2.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack[tensorflow]) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack[tensorflow]) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack[tensorflow]) (24.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[tensorflow]) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[tensorflow]) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.4.0->textattack[tensorflow])\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests (from bert-score>=0.3.5->textattack[tensorflow])\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets>=2.4.0->textattack[tensorflow])\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.4.0->textattack[tensorflow])\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[tensorflow]) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[tensorflow]) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[tensorflow]) (0.23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[tensorflow]) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack[tensorflow]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack[tensorflow]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack[tensorflow]) (2024.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (4.12.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (2.15.2)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.9.1->textattack[tensorflow]) (2.15.0)\n",
            "Collecting tensorflow>=2.9.1 (from textattack[tensorflow])\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=3.10.0 (from tensorflow>=2.9.1->textattack[tensorflow])\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes~=0.3.1 (from tensorflow>=2.9.1->textattack[tensorflow])\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard<2.17,>=2.16 (from tensorflow>=2.9.1->textattack[tensorflow])\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow>=2.9.1->textattack[tensorflow])\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack[tensorflow]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack[tensorflow]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack[tensorflow]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack[tensorflow]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.8,>=1.7.0->textattack[tensorflow])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack[tensorflow]) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack[tensorflow]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack[tensorflow]) (0.4.3)\n",
            "Collecting boto3>=1.20.27 (from flair->textattack[tensorflow])\n",
            "  Downloading boto3-1.34.120-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2 (from flair->textattack[tensorflow])\n",
            "  Downloading bpemb-0.3.5-py3-none-any.whl (19 kB)\n",
            "Collecting conllu>=4.0 (from flair->textattack[tensorflow])\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Collecting deprecated>=1.2.13 (from flair->textattack[tensorflow])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ftfy>=6.1.0 (from flair->textattack[tensorflow])\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack[tensorflow]) (5.1.0)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack[tensorflow]) (4.3.2)\n",
            "Collecting janome>=0.4.2 (from flair->textattack[tensorflow])\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect>=1.0.9 (from flair->textattack[tensorflow])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack[tensorflow]) (4.9.4)\n",
            "Collecting mpld3>=0.3 (from flair->textattack[tensorflow])\n",
            "  Downloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pptree>=3.1 (from flair->textattack[tensorflow])\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair->textattack[tensorflow])\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack[tensorflow]) (1.2.2)\n",
            "Collecting segtok>=1.5.11 (from flair->textattack[tensorflow])\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair->textattack[tensorflow])\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair->textattack[tensorflow]) (0.9.0)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair->textattack[tensorflow])\n",
            "  Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
            "Collecting urllib3<2.0.0,>=1.0.0 (from flair->textattack[tensorflow])\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia-api>=0.5.7 (from flair->textattack[tensorflow])\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting semver<4.0.0,>=3.0.0 (from flair->textattack[tensorflow])\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from language-tool-python->textattack[tensorflow]) (23.1.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from language-tool-python->textattack[tensorflow]) (0.43.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->textattack[tensorflow]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack[tensorflow]) (1.4.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->textattack[tensorflow])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting anytree (from OpenHowNet->textattack[tensorflow])\n",
            "  Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub->textattack[tensorflow]) (2.15.1)\n",
            "Collecting botocore<1.35.0,>=1.34.120 (from boto3>=1.20.27->flair->textattack[tensorflow])\n",
            "  Downloading botocore-1.34.120-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair->textattack[tensorflow])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair->textattack[tensorflow])\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair->textattack[tensorflow]) (0.1.99)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[tensorflow]) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[tensorflow]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[tensorflow]) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[tensorflow]) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[tensorflow]) (4.0.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack[tensorflow]) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack[tensorflow]) (4.12.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair->textattack[tensorflow]) (6.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow>=2.9.1->textattack[tensorflow]) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow>=2.9.1->textattack[tensorflow])\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow>=2.9.1->textattack[tensorflow])\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[tensorflow]) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (2024.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair->textattack[tensorflow]) (3.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.9.1->textattack[tensorflow]) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.9.1->textattack[tensorflow]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.9.1->textattack[tensorflow]) (3.0.3)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tf-keras>=2.14.1 (from tensorflow-hub->textattack[tensorflow])\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack[tensorflow]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack[tensorflow]) (1.3.0)\n",
            "Collecting accelerate>=0.21.0 (from transformers>=4.30.0->textattack[tensorflow])\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack[tensorflow]) (2.5)\n",
            "INFO: pip is looking at multiple versions of requests[socks] to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack[tensorflow]) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow>=2.9.1->textattack[tensorflow]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow>=2.9.1->textattack[tensorflow]) (2.16.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers>=4.30.0->textattack[tensorflow]) (5.9.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow>=2.9.1->textattack[tensorflow]) (0.1.2)\n",
            "Building wheels for collected packages: pinyin, word2number, docopt, langdetect, pptree, sqlitedict\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630475 sha256=651ee4d2c43607138b61057f32484171e872875da7b95b53398e937930cd8288\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5566 sha256=1a6ab6979cfbd1f57803259e97a61c4e8ebdc2a91207dc603c72181c482a6cdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=7021b8f3d14ce95c1381ff8e49dc939cc3e63d5930b0e6a1f5f1e1f250295b29\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=1c7e9ee686d8d66588f4e4160df6c0bd8e5fa869d2809dc6e17b1a8c59424c26\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=afa72907c8c180f7ff2eb859ccedbca1f3f14c3837201b144285751170f71042\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=84ab14dd7087029fae8346381b7667d61ccd1da8ff2acbd86aa95f810f04c31d\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built pinyin word2number docopt langdetect pptree sqlitedict\n",
            "Installing collected packages: word2number, sqlitedict, pptree, pinyin, namex, janome, docopt, xxhash, urllib3, terminaltables, tensorboardX, semver, segtok, optree, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, num2words, ml-dtypes, lru-dict, lemminflect, langdetect, jmespath, h5py, ftfy, dill, deprecated, conllu, anytree, tensorboard, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, botocore, wikipedia-api, s3transfer, OpenHowNet, nvidia-cusolver-cu12, mpld3, language-tool-python, keras, bpemb, tensorflow, datasets, boto3, tf-keras, tensorflow-text, pytorch-revgrad, accelerate, bert-score, transformer-smaller-training-vocab, flair, textattack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OpenHowNet-2.0 accelerate-0.30.1 anytree-2.12.1 bert-score-0.3.13 boto3-1.34.120 botocore-1.34.120 bpemb-0.3.5 conllu-4.5.3 datasets-2.19.2 deprecated-1.2.14 dill-0.3.8 docopt-0.6.2 flair-0.13.1 ftfy-6.2.0 h5py-3.11.0 janome-0.5.0 jmespath-1.0.1 keras-3.3.3 langdetect-1.0.9 language-tool-python-2.8 lemminflect-0.2.3 lru-dict-1.3.0 ml-dtypes-0.3.2 mpld3-0.5.10 multiprocess-0.70.16 namex-0.0.8 num2words-0.5.13 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 optree-0.11.0 pinyin-0.4.0 pptree-3.1 pytorch-revgrad-0.2.0 requests-2.32.3 s3transfer-0.10.1 segtok-1.5.11 semver-3.0.2 sqlitedict-2.1.0 tensorboard-2.16.2 tensorboardX-2.6.2.2 tensorflow-2.16.1 tensorflow-text-2.16.1 terminaltables-3.1.10 textattack-0.3.10 tf-keras-2.16.0 transformer-smaller-training-vocab-0.4.0 urllib3-1.26.18 wikipedia-api-0.6.0 word2number-1.1 xxhash-3.4.1\n",
            "Collecting phe\n",
            "  Downloading phe-1.5.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: phe\n",
            "Successfully installed phe-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise 2.2"
      ],
      "metadata": {
        "id": "9NvoaR66AG6s"
      },
      "id": "9NvoaR66AG6s"
    },
    {
      "cell_type": "code",
      "source": [
        "# Working through `TextAttack End-to-End` in documentation\n",
        "! textattack peek-dataset --dataset-from-huggingface rotten_tomatoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmOBMCqcIrUo",
        "outputId": "a9e5f9e6-aa2f-46db-9e07-e5cf787e72a8"
      },
      "id": "cmOBMCqcIrUo",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34;1mtextattack\u001b[0m: Updating TextAttack package dependencies.\n",
            "\u001b[34;1mtextattack\u001b[0m: Downloading NLTK required packages.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "2024-06-06 11:21:36.051830: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-06-06 11:21:36.113175: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-06 11:21:37.251775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading readme: 100% 7.46k/7.46k [00:00<00:00, 30.4MB/s]\n",
            "Downloading data: 100% 699k/699k [00:00<00:00, 1.27MB/s]\n",
            "Downloading data: 100% 90.0k/90.0k [00:00<00:00, 167kB/s]\n",
            "Downloading data: 100% 92.2k/92.2k [00:00<00:00, 178kB/s]\n",
            "Generating train split: 100% 8530/8530 [00:00<00:00, 225178.04 examples/s]\n",
            "Generating validation split: 100% 1066/1066 [00:00<00:00, 413037.23 examples/s]\n",
            "Generating test split: 100% 1066/1066 [00:00<00:00, 479477.54 examples/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Number of samples: \u001b[94m8530\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: Number of words per input:\n",
            "\u001b[34;1mtextattack\u001b[0m: \ttotal:   \u001b[94m157755\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tmean:    \u001b[94m18.49\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tstd:     \u001b[94m8.58\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tmin:     \u001b[94m1\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tmax:     \u001b[94m51\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: Dataset lowercased: \u001b[94mTrue\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: First sample:\n",
            "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
            "\n",
            "\u001b[34;1mtextattack\u001b[0m: Last sample:\n",
            "things really get weird , though not particularly scary : the movie is all portent and no content . \n",
            "\n",
            "\u001b[34;1mtextattack\u001b[0m: Found 2 distinct outputs.\n",
            "\u001b[34;1mtextattack\u001b[0m: Most common outputs:\n",
            "\t 1      (4265)\n",
            "\t 0      (4265)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!textattack train --model-name-or-path distilbert-base-uncased --dataset rotten_tomatoes --model-num-labels 2 --model-max-length 64 --per-device-train-batch-size 128 --num-epochs 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7M-J8HhL5Gg",
        "outputId": "f19e4c13-0114-4702-c18a-7f7cb5344d43"
      },
      "id": "Y7M-J8HhL5Gg",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-06 11:22:06.727485: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-06-06 11:22:06.787627: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-06 11:22:07.873535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading transformers AutoModelForSequenceClassification: distilbert-base-uncased\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 483/483 [00:00<00:00, 3.74MB/s]\n",
            "model.safetensors: 100% 268M/268M [00:00<00:00, 376MB/s]\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 389kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 14.0MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 39.7MB/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mvalidation\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Writing logs to ./outputs/2024-06-06-11-22-12-270093/train_log.txt.\n",
            "\u001b[34;1mtextattack\u001b[0m: Wrote original training args to ./outputs/2024-06-06-11-22-12-270093/training_args.json.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34;1mtextattack\u001b[0m: ***** Running training *****\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num examples = 8530\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num epochs = 3\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num clean epochs = 3\n",
            "\u001b[34;1mtextattack\u001b[0m:   Instantaneous batch size per device = 128\n",
            "\u001b[34;1mtextattack\u001b[0m:   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "\u001b[34;1mtextattack\u001b[0m:   Gradient accumulation steps = 1\n",
            "\u001b[34;1mtextattack\u001b[0m:   Total optimization steps = 201\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 1\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 1/3\n",
            "Loss 0.68902: 100% 67/67 [00:12<00:00,  5.43it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 53.76%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 75.89%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-06-06-11-22-12-270093/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 2\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 2/3\n",
            "Loss 0.58374: 100% 67/67 [00:10<00:00,  6.11it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 80.22%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 83.68%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-06-06-11-22-12-270093/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 3\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 3/3\n",
            "Loss 0.49955: 100% 67/67 [00:10<00:00,  6.12it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 86.41%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 84.43%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-06-06-11-22-12-270093/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: Wrote README to ./outputs/2024-06-06-11-22-12-270093/README.md.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!textattack eval --num-examples 1000 --model /content/outputs/2024-06-05-06-57-34-799210/best_model --dataset-from-huggingface rotten_tomatoes --dataset-split test"
      ],
      "metadata": {
        "id": "F1MzeHF2MMPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d390de5-3bad-45dd-ba1e-fee218313fb5"
      },
      "id": "F1MzeHF2MMPl",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-06 11:23:38.611677: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-06-06 11:23:38.671366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-06 11:23:39.743913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/textattack\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/commands/textattack_cli.py\", line 49, in main\n",
            "    func.run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/commands/eval_model_command.py\", line 103, in run\n",
            "    self.test_model_on_dataset(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/commands/eval_model_command.py\", line 47, in test_model_on_dataset\n",
            "    model = ModelArgs._create_model_from_args(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/model_args.py\", line 305, in _create_model_from_args\n",
            "    raise ValueError(f\"Error: unsupported TextAttack model {args.model}\")\n",
            "ValueError: Error: unsupported TextAttack model /content/outputs/2024-06-05-06-57-34-799210/best_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!textattack attack --recipe textfooler --num-examples 100 --model /content/outputs/2024-06-05-06-57-34-799210/best_model --dataset-from-huggingface rotten_tomatoes --dataset-split test"
      ],
      "metadata": {
        "id": "dhBXwm62MxUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4245bc4-c649-456b-eed7-4d5f2203718f"
      },
      "id": "dhBXwm62MxUB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-06 11:23:49.074717: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-06-06 11:23:49.133924: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-06 11:23:50.215620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/textattack\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/commands/textattack_cli.py\", line 49, in main\n",
            "    func.run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/commands/attack_command.py\", line 31, in run\n",
            "    model_wrapper = ModelArgs._create_model_from_args(attack_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/model_args.py\", line 305, in _create_model_from_args\n",
            "    raise ValueError(f\"Error: unsupported TextAttack model {args.model}\")\n",
            "ValueError: Error: unsupported TextAttack model /content/outputs/2024-06-05-06-57-34-799210/best_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textattack\n",
        "from textattack.attack_recipes import TextFoolerJin2019\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('/content/outputs/2024-06-06-11-22-12-270093/best_model')\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/outputs/2024-06-06-11-22-12-270093/best_model')\n",
        "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
        "\n",
        "\n",
        "dataset = HuggingFaceDataset('rotten_tomatoes', split='test')\n",
        "\n",
        "attack = TextFoolerJin2019.build(model_wrapper)\n",
        "\n",
        "\n",
        "attack.goal_function.target_class = 1\n",
        "\n",
        "\n",
        "attack_args = textattack.AttackArgs(\n",
        "    num_examples=100,\n",
        "    log_to_csv='results.csv',\n",
        "    log_to_txt='results.txt'\n",
        ")\n",
        "\n",
        "\n",
        "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
        "attacker.attack_dataset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUIA_fNJxQ5r",
        "outputId": "0c0d7955-3dfd-42c3-c865-5a2e562beb43"
      },
      "id": "SUIA_fNJxQ5r",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "textattack: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mrotten_tomatoes\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
            "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
            "100%|██████████| 481M/481M [00:44<00:00, 10.8MB/s]\n",
            "textattack: Unzipping file /root/.cache/textattack/tmp3_oc9zrw.zip to /root/.cache/textattack/word_embeddings/paragramcf.\n",
            "textattack: Successfully saved word_embeddings/paragramcf to cache.\n",
            "textattack: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
            "textattack: Logging to text file at path results.txt\n",
            "textattack: Logging to CSV at path results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack(\n",
            "  (search_method): GreedyWordSwapWIR(\n",
            "    (wir_method):  delete\n",
            "  )\n",
            "  (goal_function):  UntargetedClassification\n",
            "  (transformation):  WordSwapEmbedding(\n",
            "    (max_candidates):  50\n",
            "    (embedding):  WordEmbedding\n",
            "  )\n",
            "  (constraints): \n",
            "    (0): WordEmbeddingDistance(\n",
            "        (embedding):  WordEmbedding\n",
            "        (min_cos_sim):  0.5\n",
            "        (cased):  False\n",
            "        (include_unknown_words):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (1): PartOfSpeech(\n",
            "        (tagger_type):  nltk\n",
            "        (tagset):  universal\n",
            "        (allow_verb_noun_swap):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (2): UniversalSentenceEncoder(\n",
            "        (metric):  angular\n",
            "        (threshold):  0.840845057\n",
            "        (window_size):  15\n",
            "        (skip_text_shorter_than_window):  True\n",
            "        (compare_against_original):  False\n",
            "      )\n",
            "    (3): RepeatModification\n",
            "    (4): StopwordModification\n",
            "    (5): InputColumnModification(\n",
            "        (matching_column_labels):  ['premise', 'hypothesis']\n",
            "        (columns_to_ignore):  {'premise'}\n",
            "      )\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 2 / 0 / 1 / 3:   3%|▎         | 3/100 [01:08<36:58, 22.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 1 ---------------------------------------------\n",
            "[[Positive (94%)]] --> [[Negative (68%)]]\n",
            "\n",
            "[[lovingly]] photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n",
            "\n",
            "[[clumsily]] photographed in the manner of a golden book sprung to life , stuart little 2 manages sweetness largely without stickiness .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 2 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (93%)]]\n",
            "\n",
            "consistently [[clever]] and [[suspenseful]] .\n",
            "\n",
            "consistently [[malin]] and [[enigmatic]] .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 3 ---------------------------------------------\n",
            "[[Negative (87%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "it's like a \" big chill \" reunion of the baader-meinhof gang , only these guys are more harmless pranksters than political activists .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 4 / 0 / 2 / 6:   6%|▌         | 6/100 [01:08<17:59, 11.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 4 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (70%)]]\n",
            "\n",
            "the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with [[tremendous]] [[skill]] .\n",
            "\n",
            "the story gives ample opportunity for large-scale action and suspense , which director shekhar kapur supplies with [[stupendous]] [[jurisdictional]] .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 5 ---------------------------------------------\n",
            "[[Negative (75%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "red dragon \" never cuts corners .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 6 ---------------------------------------------\n",
            "[[Positive (62%)]] --> [[Negative (57%)]]\n",
            "\n",
            "fresnadillo has something serious to say about the [[ways]] in which extravagant chance can distort our perspective and throw us off the path of good sense .\n",
            "\n",
            "fresnadillo has something serious to say about the [[manner]] in which extravagant chance can distort our perspective and throw us off the path of good sense .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 5 / 0 / 3 / 8:   8%|▊         | 8/100 [01:09<13:14,  8.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 7 ---------------------------------------------\n",
            "[[Positive (97%)]] --> [[Negative (61%)]]\n",
            "\n",
            "throws in enough clever and [[unexpected]] [[twists]] to make the formula feel fresh .\n",
            "\n",
            "throws in enough clever and [[unwanted]] [[tendrils]] to make the formula feel fresh .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 8 ---------------------------------------------\n",
            "[[Negative (73%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "weighty and ponderous but every bit as filling as the treat of the title .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Succeeded / Failed / Skipped / Total] 6 / 0 / 3 / 9:   9%|▉         | 9/100 [01:09<11:40,  7.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 9 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (52%)]]\n",
            "\n",
            "a [[real]] audience-pleaser that will [[strike]] a [[chord]] with anyone who's ever waited in a doctor's office , emergency room , hospital bed or insurance company office .\n",
            "\n",
            "a [[actual]] audience-pleaser that will [[slugged]] a [[chords]] with anyone who's ever waited in a doctor's office , emergency room , hospital bed or insurance company office .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 8 / 0 / 3 / 11:  11%|█         | 11/100 [01:09<09:23,  6.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 10 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (96%)]]\n",
            "\n",
            "generates an [[enormous]] [[feeling]] of empathy for its [[characters]] .\n",
            "\n",
            "generates an [[dreaded]] [[foreboding]] of empathy for its [[typeface]] .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 11 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (88%)]]\n",
            "\n",
            "exposing the [[ways]] we fool ourselves is one hour photo's real [[strength]] .\n",
            "\n",
            "exposing the [[routing]] we fool ourselves is one hour photo's real [[strenght]] .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 10 / 0 / 5 / 15:  15%|█▌        | 15/100 [01:10<06:36,  4.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 12 ---------------------------------------------\n",
            "[[Positive (69%)]] --> [[Negative (54%)]]\n",
            "\n",
            "it's up to you to decide whether to admire these people's dedication to their cause or be repelled by their dogmatism , manipulativeness and narrow , fearful [[view]] of american life .\n",
            "\n",
            "it's up to you to decide whether to admire these people's dedication to their cause or be repelled by their dogmatism , manipulativeness and narrow , fearful [[consults]] of american life .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 13 ---------------------------------------------\n",
            "[[Negative (68%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "mostly , [goldbacher] just lets her complicated characters be unruly , confusing and , through it all , human .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 14 ---------------------------------------------\n",
            "[[Positive (88%)]] --> [[Negative (73%)]]\n",
            "\n",
            ". . . quite [[good]] at providing some good old [[fashioned]] spooks .\n",
            "\n",
            ". . . quite [[bestest]] at providing some good old [[moulded]] spooks .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 15 ---------------------------------------------\n",
            "[[Negative (95%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "at its worst , the movie is pretty diverting ; the pity is that it rarely achieves its best .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 11 / 0 / 5 / 16:  16%|█▌        | 16/100 [01:10<06:09,  4.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 16 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (92%)]]\n",
            "\n",
            "scherfig's light-hearted [[profile]] of [[emotional]] [[desperation]] is achingly [[honest]] and [[delightfully]] [[cheeky]] .\n",
            "\n",
            "scherfig's light-hearted [[description]] of [[affectionate]] [[discouragement]] is achingly [[fair]] and [[blithely]] [[shameless]] .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 12 / 0 / 5 / 17:  18%|█▊        | 18/100 [01:11<05:24,  3.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 17 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (64%)]]\n",
            "\n",
            "a journey [[spanning]] nearly three [[decades]] of bittersweet camaraderie and [[history]] , in which we [[feel]] that we [[truly]] know what makes holly and marina tick , and our [[hearts]] go out to them as both continue to [[negotiate]] their [[imperfect]] , love-hate [[relationship]] .\n",
            "\n",
            "a journey [[lengthened]] nearly three [[contracted]] of bittersweet camaraderie and [[bygone]] , in which we [[presume]] that we [[awfully]] know what makes holly and marina tick , and our [[cardiology]] go out to them as both continue to [[haggle]] their [[insufficient]] , love-hate [[report]] .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 14 / 0 / 5 / 19:  19%|█▉        | 19/100 [01:11<05:04,  3.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 18 ---------------------------------------------\n",
            "[[Positive (95%)]] --> [[Negative (70%)]]\n",
            "\n",
            "the [[wonderfully]] [[lush]] morvern callar is pure punk existentialism , and ms . ramsay and her co-writer , liana dognini , have dramatized the alan warner novel , which itself felt like an answer to irvine welsh's book trainspotting .\n",
            "\n",
            "the [[appallingly]] [[languid]] morvern callar is pure punk existentialism , and ms . ramsay and her co-writer , liana dognini , have dramatized the alan warner novel , which itself felt like an answer to irvine welsh's book trainspotting .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 19 ---------------------------------------------\n",
            "[[Positive (70%)]] --> [[Negative (60%)]]\n",
            "\n",
            "as it [[turns]] out , you can go [[home]] again .\n",
            "\n",
            "as it [[conversions]] out , you can go [[habitation]] again .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 15 / 0 / 5 / 20:  20%|██        | 20/100 [01:11<04:46,  3.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 20 ---------------------------------------------\n",
            "[[Positive (95%)]] --> [[Negative (71%)]]\n",
            "\n",
            "you've already seen city by the sea under a variety of titles , but it's [[worth]] yet another visit .\n",
            "\n",
            "you've already seen city by the sea under a variety of titles , but it's [[chastisement]] yet another visit .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 17 / 0 / 5 / 22:  22%|██▏       | 22/100 [01:12<04:15,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 21 ---------------------------------------------\n",
            "[[Positive (97%)]] --> [[Negative (52%)]]\n",
            "\n",
            "this kind of hands-on [[storytelling]] is ultimately what [[makes]] shanghai ghetto move beyond a [[good]] , dry , reliable textbook and what allows it to rank with its [[worthy]] predecessors .\n",
            "\n",
            "this kind of hands-on [[myth]] is ultimately what [[do]] shanghai ghetto move beyond a [[suitable]] , dry , reliable textbook and what allows it to rank with its [[legitimate]] predecessors .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 22 ---------------------------------------------\n",
            "[[Positive (68%)]] --> [[Negative (62%)]]\n",
            "\n",
            "making such a tragedy the backdrop to a love story risks trivializing it , though chouraqui no doubt intended the film to affirm love's power to help people endure almost unimaginable [[horror]] .\n",
            "\n",
            "making such a tragedy the backdrop to a love story risks trivializing it , though chouraqui no doubt intended the film to affirm love's power to help people endure almost unimaginable [[horrifying]] .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Succeeded / Failed / Skipped / Total] 18 / 0 / 5 / 23:  23%|██▎       | 23/100 [01:12<04:01,  3.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 23 ---------------------------------------------\n",
            "[[Positive (50%)]] --> [[Negative (51%)]]\n",
            "\n",
            "grown-up quibbles are beside the point here . the little girls understand , and mccracken [[knows]] that's all that matters .\n",
            "\n",
            "grown-up quibbles are beside the point here . the little girls understand , and mccracken [[know]] that's all that matters .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 20 / 0 / 5 / 25:  25%|██▌       | 25/100 [01:12<03:37,  2.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 24 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (94%)]]\n",
            "\n",
            "a [[powerful]] , [[chilling]] , and affecting study of one man's [[dying]] [[fall]] .\n",
            "\n",
            "a [[pompous]] , [[colder]] , and affecting study of one man's [[killings]] [[declined]] .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 25 ---------------------------------------------\n",
            "[[Positive (55%)]] --> [[Negative (69%)]]\n",
            "\n",
            "this is a [[fascinating]] film because there is no clear-cut hero and no all-out villain .\n",
            "\n",
            "this is a [[interesting]] film because there is no clear-cut hero and no all-out villain .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 22 / 0 / 5 / 27:  27%|██▋       | 27/100 [01:12<03:16,  2.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 26 ---------------------------------------------\n",
            "[[Positive (85%)]] --> [[Negative (82%)]]\n",
            "\n",
            "a dreadful day in irish history is given [[passionate]] , if somewhat flawed , treatment .\n",
            "\n",
            "a dreadful day in irish history is given [[greedy]] , if somewhat flawed , treatment .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 27 ---------------------------------------------\n",
            "[[Positive (93%)]] --> [[Negative (89%)]]\n",
            "\n",
            ". . . a [[good]] film that must have baffled the folks in the marketing department .\n",
            "\n",
            ". . . a [[ok]] film that must have baffled the folks in the marketing department .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 23 / 0 / 6 / 29:  29%|██▉       | 29/100 [01:13<02:58,  2.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 28 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (55%)]]\n",
            "\n",
            ". . . is [[funny]] in the [[way]] that makes you ache with sadness ( the way chekhov is funny ) , [[profound]] without ever being self-important , [[warm]] without ever succumbing to sentimentality .\n",
            "\n",
            ". . . is [[outlandish]] in the [[trajectories]] that makes you ache with sadness ( the way chekhov is funny ) , [[shum]] without ever being self-important , [[tepid]] without ever succumbing to sentimentality .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 29 ---------------------------------------------\n",
            "[[Negative (96%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "devotees of star trek ii : the wrath of khan will feel a nagging sense of deja vu , and the grandeur of the best next generation episodes is lacking .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 25 / 1 / 7 / 33:  33%|███▎      | 33/100 [01:14<02:30,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 30 ---------------------------------------------\n",
            "[[Positive (99%)]] --> [[[FAILED]]]\n",
            "\n",
            "a soul-stirring documentary about the israeli/palestinian conflict as revealed through the eyes of some children who remain curious about each other against all odds .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 31 ---------------------------------------------\n",
            "[[Positive (51%)]] --> [[Negative (90%)]]\n",
            "\n",
            "what's so [[striking]] about jolie's performance is that she never lets her character become a caricature -- not even with that radioactive hair .\n",
            "\n",
            "what's so [[staggering]] about jolie's performance is that she never lets her character become a caricature -- not even with that radioactive hair .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 32 ---------------------------------------------\n",
            "[[Negative (77%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "the main story . . . is compelling enough , but it's difficult to shrug off the annoyance of that chatty fish .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 33 ---------------------------------------------\n",
            "[[Positive (95%)]] --> [[Negative (94%)]]\n",
            "\n",
            "the performances are [[immaculate]] , with roussillon providing comic relief .\n",
            "\n",
            "the performances are [[faultless]] , with roussillon providing comic relief .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 27 / 1 / 7 / 35:  36%|███▌      | 36/100 [01:14<02:13,  2.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 34 ---------------------------------------------\n",
            "[[Positive (97%)]] --> [[Negative (51%)]]\n",
            "\n",
            "kinnear . . . [[gives]] his best [[screen]] performance with an oddly [[winning]] [[portrayal]] of one of life's [[ultimate]] [[losers]] .\n",
            "\n",
            "kinnear . . . [[stipulates]] his best [[screening]] performance with an oddly [[earns]] [[similarity]] of one of life's [[finale]] [[fuckers]] .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 35 ---------------------------------------------\n",
            "[[Positive (57%)]] --> [[Negative (59%)]]\n",
            "\n",
            "hugh grant , who has a good line in charm , [[has]] never been more charming than in about a boy .\n",
            "\n",
            "hugh grant , who has a good line in charm , [[was]] never been more charming than in about a boy .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 28 / 1 / 8 / 37:  37%|███▋      | 37/100 [01:14<02:07,  2.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 36 ---------------------------------------------\n",
            "[[Positive (94%)]] --> [[Negative (72%)]]\n",
            "\n",
            "there's a lot of tooth in roger dodger . but what's [[nice]] is that there's a casual intelligence that permeates the script .\n",
            "\n",
            "there's a lot of tooth in roger dodger . but what's [[leggy]] is that there's a casual intelligence that permeates the script .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 37 ---------------------------------------------\n",
            "[[Negative (74%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "reminiscent of alfred hitchcock's thrillers , most of the scary parts in 'signs' occur while waiting for things to happen .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 29 / 1 / 8 / 38:  38%|███▊      | 38/100 [01:15<02:02,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 38 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (61%)]]\n",
            "\n",
            "one of the [[best]] looking and [[stylish]] [[animated]] movies in quite a while . . .\n",
            "\n",
            "one of the [[strictest]] looking and [[fashionable]] [[abetted]] movies in quite a while . . .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 31 / 1 / 8 / 40:  40%|████      | 40/100 [01:15<01:53,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 39 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (80%)]]\n",
            "\n",
            "its use of the thriller form to examine the labyrinthine ways in which people's lives cross and change , buffeted by events seemingly out of their control , is [[intriguing]] , [[provocative]] stuff .\n",
            "\n",
            "its use of the thriller form to examine the labyrinthine ways in which people's lives cross and change , buffeted by events seemingly out of their control , is [[disconcerting]] , [[incite]] stuff .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 40 ---------------------------------------------\n",
            "[[Positive (88%)]] --> [[Negative (63%)]]\n",
            "\n",
            "denver [[should]] not get the first and last look at one of the most triumphant performances of vanessa redgrave's career . it deserves to be seen everywhere .\n",
            "\n",
            "denver [[woud]] not get the first and last look at one of the most triumphant performances of vanessa redgrave's career . it deserves to be seen everywhere .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 33 / 1 / 8 / 42:  42%|████▏     | 42/100 [01:15<01:44,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 41 ---------------------------------------------\n",
            "[[Positive (62%)]] --> [[Negative (61%)]]\n",
            "\n",
            "you needn't be steeped in '50s sociology , pop culture or movie lore to appreciate the emotional depth of haynes' work . [[though]] haynes' style apes films from the period . . . its message is not rooted in that decade .\n",
            "\n",
            "you needn't be steeped in '50s sociology , pop culture or movie lore to appreciate the emotional depth of haynes' work . [[albeit]] haynes' style apes films from the period . . . its message is not rooted in that decade .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 42 ---------------------------------------------\n",
            "[[Positive (94%)]] --> [[Negative (77%)]]\n",
            "\n",
            "waiting for godard can be [[fruitful]] : 'in praise of love' is the director's epitaph for himself .\n",
            "\n",
            "waiting for godard can be [[salubrious]] : 'in praise of love' is the director's epitaph for himself .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 35 / 1 / 8 / 44:  44%|████▍     | 44/100 [01:15<01:36,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 43 ---------------------------------------------\n",
            "[[Positive (96%)]] --> [[Negative (86%)]]\n",
            "\n",
            "a gangster movie with the capacity to [[surprise]] .\n",
            "\n",
            "a gangster movie with the capacity to [[flabbergasted]] .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 44 ---------------------------------------------\n",
            "[[Positive (78%)]] --> [[Negative (87%)]]\n",
            "\n",
            "the film has a laundry list of minor shortcomings , but the numerous scenes of gory mayhem are [[worth]] the price of admission . . . if \" gory mayhem \" is your idea of a good time .\n",
            "\n",
            "the film has a laundry list of minor shortcomings , but the numerous scenes of gory mayhem are [[priceless]] the price of admission . . . if \" gory mayhem \" is your idea of a good time .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 37 / 1 / 8 / 46:  46%|████▌     | 46/100 [01:16<01:29,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 45 ---------------------------------------------\n",
            "[[Positive (69%)]] --> [[Negative (85%)]]\n",
            "\n",
            "if not a home run , then at least a [[solid]] base hit .\n",
            "\n",
            "if not a home run , then at least a [[beefy]] base hit .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 46 ---------------------------------------------\n",
            "[[Positive (86%)]] --> [[Negative (81%)]]\n",
            "\n",
            "goldmember is [[funny]] enough to justify the embarrassment of bringing a barf bag to the moviehouse .\n",
            "\n",
            "goldmember is [[comical]] enough to justify the embarrassment of bringing a barf bag to the moviehouse .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Succeeded / Failed / Skipped / Total] 38 / 1 / 8 / 47:  47%|████▋     | 47/100 [01:16<01:26,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 47 ---------------------------------------------\n",
            "[[Positive (92%)]] --> [[Negative (85%)]]\n",
            "\n",
            ". . . a fairly disposable yet still [[entertaining]] b picture .\n",
            "\n",
            ". . . a fairly disposable yet still [[droll]] b picture .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 40 / 1 / 8 / 49:  49%|████▉     | 49/100 [01:16<01:19,  1.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 48 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (55%)]]\n",
            "\n",
            "it may not be particularly [[innovative]] , but the film's crisp , unaffected style and air of [[gentle]] [[longing]] [[make]] it unexpectedly [[rewarding]] .\n",
            "\n",
            "it may not be particularly [[revolutionary]] , but the film's crisp , unaffected style and air of [[mild]] [[vacuuming]] [[rendered]] it unexpectedly [[bounties]] .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 49 ---------------------------------------------\n",
            "[[Positive (96%)]] --> [[Negative (76%)]]\n",
            "\n",
            "the film [[truly]] does rescue [the funk brothers] from motown's shadows . it's about time .\n",
            "\n",
            "the film [[awfully]] does rescue [the funk brothers] from motown's shadows . it's about time .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 41 / 1 / 9 / 51:  51%|█████     | 51/100 [01:17<01:14,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 50 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (52%)]]\n",
            "\n",
            "drawing on an [[irresistible]] , languid romanticism , byler [[reveals]] the [[ways]] in which a sultry evening or a beer-fueled afternoon in the sun can [[inspire]] even the most retiring heart to venture forth .\n",
            "\n",
            "drawing on an [[stupendous]] , languid romanticism , byler [[divulged]] the [[menu]] in which a sultry evening or a beer-fueled afternoon in the sun can [[inspiration]] even the most retiring heart to venture forth .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 51 ---------------------------------------------\n",
            "[[Negative (88%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "works because we're never sure if ohlinger's on the level or merely a dying , delusional man trying to get into the history books before he croaks .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 42 / 1 / 9 / 52:  52%|█████▏    | 52/100 [01:17<01:11,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 52 ---------------------------------------------\n",
            "[[Positive (84%)]] --> [[Negative (51%)]]\n",
            "\n",
            "[scherfig] has made a [[movie]] that will leave you wondering about the characters' [[lives]] after the [[clever]] credits roll .\n",
            "\n",
            "[scherfig] has made a [[cine]] that will leave you wondering about the characters' [[vie]] after the [[cleverer]] credits roll .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 43 / 1 / 9 / 53:  54%|█████▍    | 54/100 [01:17<01:06,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 53 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (53%)]]\n",
            "\n",
            "a [[heady]] , [[biting]] , be-bop ride through nighttime manhattan , a loquacious videologue of the [[modern]] male and the lengths to which he'll go to [[weave]] a protective cocoon around his own ego .\n",
            "\n",
            "a [[disordered]] , [[gnawing]] , be-bop ride through nighttime manhattan , a loquacious videologue of the [[upgraded]] male and the lengths to which he'll go to [[sewn]] a protective cocoon around his own ego .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 54 ---------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 46 / 1 / 9 / 56:  56%|█████▌    | 56/100 [01:18<01:01,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[Positive (97%)]] --> [[Negative (52%)]]\n",
            "\n",
            "skin of man gets a few cheap shocks from its kids-in-peril theatrics , but it also [[taps]] into the [[primal]] fears of young people trying to cope with the [[mysterious]] and brutal nature of adults .\n",
            "\n",
            "skin of man gets a few cheap shocks from its kids-in-peril theatrics , but it also [[faucets]] into the [[rudimentary]] fears of young people trying to cope with the [[inscrutable]] and brutal nature of adults .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 55 ---------------------------------------------\n",
            "[[Positive (77%)]] --> [[Negative (51%)]]\n",
            "\n",
            "the piano teacher is not an easy film . it forces you to watch people doing unpleasant things to each other and themselves , and it maintains a [[cool]] distance from its material that is deliberately unsettling .\n",
            "\n",
            "the piano teacher is not an easy film . it forces you to watch people doing unpleasant things to each other and themselves , and it maintains a [[super]] distance from its material that is deliberately unsettling .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 56 ---------------------------------------------\n",
            "[[Positive (96%)]] --> [[Negative (90%)]]\n",
            "\n",
            "as [[refreshing]] as a drink from a woodland stream .\n",
            "\n",
            "as [[retrofit]] as a drink from a woodland stream .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 48 / 1 / 9 / 58:  58%|█████▊    | 58/100 [01:18<00:56,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 57 ---------------------------------------------\n",
            "[[Positive (61%)]] --> [[Negative (95%)]]\n",
            "\n",
            "williams absolutely nails sy's queasy infatuation and overall [[strangeness]] .\n",
            "\n",
            "williams absolutely nails sy's queasy infatuation and overall [[ennui]] .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 58 ---------------------------------------------\n",
            "[[Positive (75%)]] --> [[Negative (74%)]]\n",
            "\n",
            "can i admit xxx is as deep as a petri dish and as well-characterized as a telephone book but [[still]] say it was a guilty [[pleasure]] ?\n",
            "\n",
            "can i admit xxx is as deep as a petri dish and as well-characterized as a telephone book but [[even]] say it was a guilty [[amusement]] ?\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 51 / 1 / 9 / 61:  61%|██████    | 61/100 [01:18<00:50,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 59 ---------------------------------------------\n",
            "[[Positive (94%)]] --> [[Negative (85%)]]\n",
            "\n",
            "while it's nothing we haven't seen before from murphy , i spy is still [[fun]] and [[enjoyable]] and so aggressively silly that it's more than a worthwhile effort .\n",
            "\n",
            "while it's nothing we haven't seen before from murphy , i spy is still [[amusement]] and [[cosy]] and so aggressively silly that it's more than a worthwhile effort .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 60 ---------------------------------------------\n",
            "[[Positive (68%)]] --> [[Negative (62%)]]\n",
            "\n",
            "by the time it ends in a rush of sequins , flashbulbs , blaring brass and back-stabbing babes , it has said [[plenty]] about how show business has infiltrated every corner of society -- and not always for the better .\n",
            "\n",
            "by the time it ends in a rush of sequins , flashbulbs , blaring brass and back-stabbing babes , it has said [[lots]] about how show business has infiltrated every corner of society -- and not always for the better .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 61 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (77%)]]\n",
            "\n",
            "an [[intimate]] contemplation of two marvelously messy lives .\n",
            "\n",
            "an [[squeamish]] contemplation of two marvelously messy lives .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 52 / 1 / 9 / 62:  62%|██████▏   | 62/100 [01:18<00:48,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 62 ---------------------------------------------\n",
            "[[Positive (56%)]] --> [[Negative (56%)]]\n",
            "\n",
            "rarely has skin looked as [[beautiful]] , desirable , even delectable , as it does in trouble every day .\n",
            "\n",
            "rarely has skin looked as [[nice]] , desirable , even delectable , as it does in trouble every day .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 55 / 1 / 9 / 65:  65%|██████▌   | 65/100 [01:19<00:42,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 63 ---------------------------------------------\n",
            "[[Positive (97%)]] --> [[Negative (57%)]]\n",
            "\n",
            "this is one of those rare docs that [[paints]] a [[grand]] picture of an [[era]] and [[makes]] the [[journey]] feel like a [[party]] .\n",
            "\n",
            "this is one of those rare docs that [[paint]] a [[gargantuan]] picture of an [[timeframe]] and [[ai]] the [[outing]] feel like a [[portions]] .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 64 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (94%)]]\n",
            "\n",
            "[[poignant]] if familiar story of a young person suspended between two cultures .\n",
            "\n",
            "[[dreaded]] if familiar story of a young person suspended between two cultures .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 65 ---------------------------------------------\n",
            "[[Positive (93%)]] --> [[Negative (93%)]]\n",
            "\n",
            "a [[metaphor]] for a modern-day urban china searching for its identity .\n",
            "\n",
            "a [[cliché]] for a modern-day urban china searching for its identity .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 57 / 1 / 9 / 67:  67%|██████▋   | 67/100 [01:19<00:39,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 66 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (78%)]]\n",
            "\n",
            "for all its brooding quality , ash wednesday is [[suspenseful]] and ultimately unpredictable , with a [[sterling]] ensemble cast .\n",
            "\n",
            "for all its brooding quality , ash wednesday is [[cliffhanger]] and ultimately unpredictable , with a [[stirling]] ensemble cast .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 67 ---------------------------------------------\n",
            "[[Positive (93%)]] --> [[Negative (77%)]]\n",
            "\n",
            "an odd [[drama]] set in the [[world]] of lingerie models and bar dancers in the midwest that held my interest precisely because it didn't try to .\n",
            "\n",
            "an odd [[cataclysmic]] set in the [[globo]] of lingerie models and bar dancers in the midwest that held my interest precisely because it didn't try to .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 59 / 1 / 9 / 69:  69%|██████▉   | 69/100 [01:20<00:35,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 68 ---------------------------------------------\n",
            "[[Positive (83%)]] --> [[Negative (54%)]]\n",
            "\n",
            "the film feels uncomfortably real , its language and locations bearing the [[unmistakable]] stamp of authority .\n",
            "\n",
            "the film feels uncomfortably real , its language and locations bearing the [[discernible]] stamp of authority .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 69 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (88%)]]\n",
            "\n",
            "despite its faults , gangs [[excels]] in spectacle and pacing .\n",
            "\n",
            "despite its faults , gangs [[overwhelms]] in spectacle and pacing .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 61 / 1 / 9 / 71:  71%|███████   | 71/100 [01:20<00:32,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 70 ---------------------------------------------\n",
            "[[Positive (76%)]] --> [[Negative (54%)]]\n",
            "\n",
            "[[entertaining]] despite its one-joke premise with the thesis that women from venus and men from mars can indeed get together .\n",
            "\n",
            "[[amusing]] despite its one-joke premise with the thesis that women from venus and men from mars can indeed get together .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 71 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (92%)]]\n",
            "\n",
            "a tightly directed , [[highly]] professional film that's old-fashioned in all the best possible ways .\n",
            "\n",
            "a tightly directed , [[excessively]] professional film that's old-fashioned in all the best possible ways .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 63 / 1 / 9 / 73:  73%|███████▎  | 73/100 [01:20<00:29,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 72 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (53%)]]\n",
            "\n",
            "it's [[dark]] but has [[wonderfully]] [[funny]] [[moments]] ; you [[care]] about the [[characters]] ; and the [[action]] and special effects are first-rate .\n",
            "\n",
            "it's [[dismal]] but has [[freakishly]] [[comical]] [[mins]] ; you [[zorg]] about the [[specification]] ; and the [[jobs]] and special effects are first-rate .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 73 ---------------------------------------------\n",
            "[[Positive (93%)]] --> [[Negative (57%)]]\n",
            "\n",
            "in visual fertility treasure planet rivals the [[top]] japanese animations of recent [[vintage]] .\n",
            "\n",
            "in visual fertility treasure planet rivals the [[supremo]] japanese animations of recent [[reaping]] .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 65 / 1 / 9 / 75:  75%|███████▌  | 75/100 [01:21<00:27,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 74 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (95%)]]\n",
            "\n",
            "[[enormously]] [[enjoyable]] , high-adrenaline documentary .\n",
            "\n",
            "[[terribly]] [[cosy]] , high-adrenaline documentary .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 75 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (92%)]]\n",
            "\n",
            "buy is an [[accomplished]] actress , and this is a big , [[juicy]] role .\n",
            "\n",
            "buy is an [[ended]] actress , and this is a big , [[crusty]] role .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 66 / 1 / 9 / 76:  76%|███████▌  | 76/100 [01:21<00:25,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 76 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (54%)]]\n",
            "\n",
            "it [[works]] its magic with such [[exuberance]] and passion that the film's length becomes a part of its [[fun]] .\n",
            "\n",
            "it [[functioned]] its magic with such [[naiveté]] and passion that the film's length becomes a part of its [[banter]] .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 68 / 1 / 9 / 78:  78%|███████▊  | 78/100 [01:22<00:23,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 77 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (83%)]]\n",
            "\n",
            "[[beautifully]] [[crafted]] and [[brutally]] [[honest]] , promises [[offers]] an unexpected [[window]] into the complexities of the middle east [[struggle]] and into the [[humanity]] of its people .\n",
            "\n",
            "[[impossibly]] [[prepped]] and [[grossly]] [[franc]] , promises [[offering]] an unexpected [[beaker]] into the complexities of the middle east [[tussle]] and into the [[humans]] of its people .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 78 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (58%)]]\n",
            "\n",
            "an old-fashioned but emotionally [[stirring]] adventure tale of the kind they rarely make anymore .\n",
            "\n",
            "an old-fashioned but emotionally [[irate]] adventure tale of the kind they rarely make anymore .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Succeeded / Failed / Skipped / Total] 69 / 1 / 9 / 79:  79%|███████▉  | 79/100 [01:22<00:21,  1.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 79 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (93%)]]\n",
            "\n",
            "charlotte sometimes is a [[gem]] . it's always [[enthralling]] .\n",
            "\n",
            "charlotte sometimes is a [[bling]] . it's always [[scintillating]] .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 70 / 2 / 9 / 81:  81%|████████  | 81/100 [01:22<00:19,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 80 ---------------------------------------------\n",
            "[[Positive (93%)]] --> [[[FAILED]]]\n",
            "\n",
            "in my opinion , analyze that is not as funny or entertaining as analyze this , but it is a respectable sequel .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 81 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (93%)]]\n",
            "\n",
            "a [[remarkable]] film by bernard rose .\n",
            "\n",
            "a [[whopping]] film by bernard rose .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 72 / 2 / 9 / 83:  83%|████████▎ | 83/100 [01:23<00:17,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 82 ---------------------------------------------\n",
            "[[Positive (63%)]] --> [[Negative (51%)]]\n",
            "\n",
            "zhuangzhuang [[creates]] delicate balance of style , text , and subtext that's so simple and precise that anything discordant would topple the balance , but against all odds , nothing does .\n",
            "\n",
            "zhuangzhuang [[produces]] delicate balance of style , text , and subtext that's so simple and precise that anything discordant would topple the balance , but against all odds , nothing does .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 83 ---------------------------------------------\n",
            "[[Positive (90%)]] --> [[Negative (51%)]]\n",
            "\n",
            "a much more [[successful]] translation than its most famous previous film adaptation , writer-director anthony friedman's similarly updated 1970 british production .\n",
            "\n",
            "a much more [[conducive]] translation than its most famous previous film adaptation , writer-director anthony friedman's similarly updated 1970 british production .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 74 / 2 / 9 / 85:  85%|████████▌ | 85/100 [01:23<00:14,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 84 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (96%)]]\n",
            "\n",
            "an [[original]] and [[highly]] cerebral examination of the psychopathic mind\n",
            "\n",
            "an [[rudimentary]] and [[excessively]] cerebral examination of the psychopathic mind\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 85 ---------------------------------------------\n",
            "[[Positive (95%)]] --> [[Negative (78%)]]\n",
            "\n",
            "michel piccoli's [[moving]] performance is this films reason for being .\n",
            "\n",
            "michel piccoli's [[displaced]] performance is this films reason for being .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 76 / 2 / 9 / 87:  87%|████████▋ | 87/100 [01:23<00:12,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 86 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (84%)]]\n",
            "\n",
            "a [[captivating]] and [[intimate]] [[study]] about [[dying]] and [[loving]] . . .\n",
            "\n",
            "a [[hallucinatory]] and [[cosy]] [[scrutinized]] about [[assassinating]] and [[affectionate]] . . .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 87 ---------------------------------------------\n",
            "[[Positive (96%)]] --> [[Negative (89%)]]\n",
            "\n",
            "this is an [[elegantly]] [[balanced]] movie -- every member of the ensemble has something fascinating to do -- that doesn't reveal even a hint of artifice .\n",
            "\n",
            "this is an [[prettily]] [[balancing]] movie -- every member of the ensemble has something fascinating to do -- that doesn't reveal even a hint of artifice .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 77 / 2 / 9 / 88:  88%|████████▊ | 88/100 [01:24<00:11,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 88 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (50%)]]\n",
            "\n",
            "[grant] goes beyond his usual fluttering and [[stammering]] and [[captures]] the [[soul]] of a man in pain who gradually comes to recognize it and deal with it .\n",
            "\n",
            "[grant] goes beyond his usual fluttering and [[stutterer]] and [[incarcerate]] the [[wits]] of a man in pain who gradually comes to recognize it and deal with it .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 79 / 2 / 10 / 91:  91%|█████████ | 91/100 [01:24<00:08,  1.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 89 ---------------------------------------------\n",
            "[[Positive (97%)]] --> [[Negative (61%)]]\n",
            "\n",
            "a high-spirited [[buddy]] [[movie]] about the [[reunion]] of [[berlin]] [[anarchists]] who [[face]] [[arrest]] 15 [[years]] after their [[crime]] .\n",
            "\n",
            "a high-spirited [[fella]] [[video]] about the [[pooling]] of [[germania]] [[fascists]] who [[confrontation]] [[halts]] 15 [[leto]] after their [[felony]] .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 90 ---------------------------------------------\n",
            "[[Negative (79%)]] --> [[[SKIPPED]]]\n",
            "\n",
            "about the best thing you could say about narc is that it's a rock-solid little genre picture . whether you like it or not is basically a matter of taste .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 91 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (83%)]]\n",
            "\n",
            "an involving , [[inspirational]] [[drama]] that sometimes falls prey to its sob-story trappings .\n",
            "\n",
            "an involving , [[incite]] [[cataclysmic]] that sometimes falls prey to its sob-story trappings .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 80 / 2 / 10 / 92:  92%|█████████▏| 92/100 [01:24<00:07,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 92 ---------------------------------------------\n",
            "[[Positive (97%)]] --> [[Negative (66%)]]\n",
            "\n",
            "some of the most [[inventive]] silliness you are likely to witness in a movie theatre for some time .\n",
            "\n",
            "some of the most [[contrivance]] silliness you are likely to witness in a movie theatre for some time .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Succeeded / Failed / Skipped / Total] 81 / 2 / 10 / 93:  93%|█████████▎| 93/100 [01:25<00:06,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 93 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (51%)]]\n",
            "\n",
            "canadian [[filmmaker]] gary burns' [[inventive]] and mordantly [[humorous]] [[take]] on the soullessness of [[work]] in the [[city]] .\n",
            "\n",
            "canadian [[screenwriter]] gary burns' [[inventor]] and mordantly [[mockery]] [[toma]] on the soullessness of [[collaborated]] in the [[stadt]] .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 82 / 2 / 10 / 94:  94%|█████████▍| 94/100 [01:25<00:05,  1.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 94 ---------------------------------------------\n",
            "[[Positive (98%)]] --> [[Negative (63%)]]\n",
            "\n",
            "a rollicking [[ride]] , with jaw-dropping action sequences , striking villains , a [[gorgeous]] color palette , astounding technology , [[stirring]] music and a boffo last hour that leads up to a strangely sinister happy ending .\n",
            "\n",
            "a rollicking [[wrinkle]] , with jaw-dropping action sequences , striking villains , a [[leggy]] color palette , astounding technology , [[agitation]] music and a boffo last hour that leads up to a strangely sinister happy ending .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 85 / 2 / 10 / 97:  97%|█████████▋| 97/100 [01:26<00:02,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 95 ---------------------------------------------\n",
            "[[Positive (99%)]] --> [[Negative (62%)]]\n",
            "\n",
            "everyone's [[insecure]] in lovely and [[amazing]] , a [[poignant]] and wryly amusing [[film]] about mothers , daughters and their relationships .\n",
            "\n",
            "everyone's [[substandard]] in lovely and [[whopping]] , a [[worrisome]] and wryly amusing [[theatres]] about mothers , daughters and their relationships .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 96 ---------------------------------------------\n",
            "[[Positive (60%)]] --> [[Negative (70%)]]\n",
            "\n",
            "the closest thing to the [[experience]] of space travel\n",
            "\n",
            "the closest thing to the [[piloting]] of space travel\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 97 ---------------------------------------------\n",
            "[[Positive (96%)]] --> [[Negative (97%)]]\n",
            "\n",
            "full of [[surprises]] .\n",
            "\n",
            "full of [[stupor]] .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 87 / 2 / 10 / 99:  99%|█████████▉| 99/100 [01:26<00:00,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 98 ---------------------------------------------\n",
            "[[Positive (92%)]] --> [[Negative (51%)]]\n",
            "\n",
            "connoisseurs of chinese film will be [[pleased]] to discover that tian's meticulous [[talent]] has not [[withered]] during his enforced hiatus .\n",
            "\n",
            "connoisseurs of chinese film will be [[flattered]] to discover that tian's meticulous [[staff]] has not [[forlorn]] during his enforced hiatus .\n",
            "\n",
            "\n",
            "--------------------------------------------- Result 99 ---------------------------------------------\n",
            "[[Positive (93%)]] --> [[Negative (58%)]]\n",
            "\n",
            "if you can push on through the slow spots , you'll be [[rewarded]] with some [[fine]] [[acting]] .\n",
            "\n",
            "if you can push on through the slow spots , you'll be [[recompense]] with some [[wondrous]] [[interim]] .\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Succeeded / Failed / Skipped / Total] 88 / 2 / 10 / 100: 100%|██████████| 100/100 [01:26<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------- Result 100 ---------------------------------------------\n",
            "[[Positive (50%)]] --> [[Negative (69%)]]\n",
            "\n",
            "an unusually dry-eyed , even analytical [[approach]] to material that is generally played for maximum moisture .\n",
            "\n",
            "an unusually dry-eyed , even analytical [[approaches]] to material that is generally played for maximum moisture .\n",
            "\n",
            "\n",
            "\n",
            "+-------------------------------+--------+\n",
            "| Attack Results                |        |\n",
            "+-------------------------------+--------+\n",
            "| Number of successful attacks: | 88     |\n",
            "| Number of failed attacks:     | 2      |\n",
            "| Number of skipped attacks:    | 10     |\n",
            "| Original accuracy:            | 90.0%  |\n",
            "| Accuracy under attack:        | 2.0%   |\n",
            "| Attack success rate:          | 97.78% |\n",
            "| Average perturbed word %:     | 14.87% |\n",
            "| Average num. words per input: | 18.45  |\n",
            "| Avg num queries:              | 82.56  |\n",
            "+-------------------------------+--------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4fed3c040>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011e5840>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7ad4ff13f400>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc1cc0>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7ad50bdc22f0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc2380>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4ff13f2b0>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7ad4fe360c40>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc2110>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc1a80>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f3084e50>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f3086b90>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7ad4f30858a0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc14e0>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7ad50bdc1720>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f3087d60>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303d120>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303d900>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303eb90>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303d750>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303f160>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303cf70>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303ccd0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc1270>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc05b0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc0d00>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc0850>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4fed17730>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7ad4f303c1f0>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7ad4fed3ffd0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011f4ca0>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7ad500509a80>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad500d29840>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad69677f460>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4ff13f880>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4ff13f4f0>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7ad4f3142ef0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad500548430>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad500548ac0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4ff13faf0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5005484c0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50052aec0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5005481f0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad500548550>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50052ac20>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5005482e0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50052b190>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50052b610>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50052ba00>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad500548070>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7ad5005489a0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50052abc0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f31431c0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50052b4f0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50052b9a0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50054ba00>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4fe363af0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50052ac80>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f3143d90>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f31426b0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f3143910>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f31411e0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011c53c0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc1870>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011c51b0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc1f90>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011c5510>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303d510>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303dc60>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011c5990>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc17b0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50052bb50>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011c6dd0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc0e20>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011c6e60>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f3140550>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011c5570>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc2980>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011c5420>,\n",
              " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x7ad5011c56c0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011c4f70>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f2e2ae60>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f2e29ab0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f3142ce0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc2530>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad5011c4ee0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303f700>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc2500>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc1960>,\n",
              " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x7ad50bdc2680>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50bdc11e0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f2eea560>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50c1dad40>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50c1db1c0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50c1d9bd0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad50c1da980>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f2ee9fc0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4fe361ab0>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4fe361870>,\n",
              " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x7ad4f303cbe0>]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise 3.3"
      ],
      "metadata": {
        "id": "E6GvUazQ6XFj"
      },
      "id": "E6GvUazQ6XFj"
    },
    {
      "cell_type": "code",
      "source": [
        "import phe as paillier\n",
        "\n",
        "pub, pri = paillier.generate_paillier_keypair()\n",
        "\n",
        "a = 5\n",
        "b = 3\n",
        "\n",
        "encrypted_a = pub.encrypt(a)\n",
        "encrypted_b = pub.encrypt(b)\n",
        "\n",
        "print(encrypted_a.ciphertext())\n",
        "print(encrypted_b.ciphertext())\n",
        "\n",
        "# homomorphic addition\n",
        "encrypted_sum = encrypted_a + encrypted_b\n",
        "\n",
        "decrypted_sum = pri.decrypt(encrypted_sum)\n",
        "\n",
        "(a + b)  == decrypted_sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlkRSmHPqYGt",
        "outputId": "9425eba4-25f5-45a4-edf1-81c9177ade3b"
      },
      "id": "WlkRSmHPqYGt",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10078687956001611611851974358198015216924975838407016040477564867808397104170890912740983409653273932438051414527602477708621319413228986221766019638298183773047532413803929468975474242205015612790386565058792541407190072949827075259440106812538564626310442359452775821499919612221775710292133775199770272116552176627654628804865102759791350451764750076274712474578324314170329230365642741177610280077137249047815023042133784076851569916006339998821322883915952926459284826937205220266965731911970132294712331425347242973660084409091604073604794566517644241327958988255230451863487423376296207361561371711587591665888201871990526564427918909572728608581860176273187790984921976129121775613662101329580261293004924344629448590147409556733900741467759879019873288456041598582401614165603567804646324708723929324802531978056257579126993694970836152174775101309707621862813600251707605202374528568649710289563378068758646778455528011687944329468418245928147690932598302323101448464635988000818917582534037046434447457347922342580423798341408240881889809779005465281267222822260821092058479728613382019767570753355621300268287434754444720130858224247892333771632293413809259102288058230060862507242143232714323422837352954185198807761874799001105459843685460772647054093028631084695272029322794822033099670608623061338648310333440103200286661891263547105856110089295624494016028379789536284024251486156234207602886460200326220915704721105967963677478433152949663379917287140897971373461259656653181420344649252993528971633739084988871600043283875660362335437245353493625461082644451709645898408233211040848564059039015065428423521182296193765358674909347950067762900260643045026153059137537244144667233806269970606753073138402745679076524482770753687652295360825639601815413559799998938181953658886928306185228765503779009863039700179151663672805944692582\n",
            "10502405772346302540207084062264205531835803420719142935410190027668707686203680257301185524474590892622415701092175331274174658761638849577691198059350650743232204033453735639537330105359739606151219419343568727427696977925825520854509326806385677729422007869764689066868065853107140337481758460059022332756046558721897345467856510359742100767354801398893758963767458173150151044655568326279978366891551059897528227949659863838075980116474971965939065480680964397646142752217054822135992068343927604006092658671243009669279095164304981934352925216208524492477760953179689561446357337067843231147536420250271743856656687658254606024841700540032616346451301458115267045088885658748725097195110662963725649220086956688210914307837922249404907036106636598995730449001246314722120074220841345300117177693814101798122412937400349105326841199297598839809945619524956872066279590270264359425363788241076285409326545750645380485966857444432299453295186730716577156601530770097243686542725967264486290131913645578400089511494257109022876142162999548933062652473266786751665467717864579685657791497041261207480892099623522239375994901778860509930241559027961117483279337550962043511692096504250823086979929554183675620051780518032443271559960979798826526878349069698082322205743092514590356924392329338234461341239035484949895681851131324929144445662904094434226289099226619715456259460867261619265675631097630826420986360650270823110965789108025453062844532077615966279077558985739210452971851443479854931356619420819630174700377558655766649833984439301231188738534895706886076891395090711282482159844685170264613327265640189195802373783166567567117631824580994737380777243017683621253849676260361292510988657635407385927267222881319573778103352288937002765384134016728591894170030716244182051308149410703738360217943722519020888143997455111892920807176526630\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Excercise 4.4"
      ],
      "metadata": {
        "id": "vSHN2EVYdzTN"
      },
      "id": "vSHN2EVYdzTN"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Iris dataset from sklearn\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = X[:, 2]\n",
        "\n",
        "# Remove column to create uncertainty\n",
        "X = np.delete(X, 2, axis=1)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preparing Data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n"
      ],
      "metadata": {
        "id": "oxQUbvhldaze"
      },
      "id": "oxQUbvhldaze",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3, 30)\n",
        "        self.fc2 = nn.Linear(30, 30)\n",
        "        # predicting the mean\n",
        "        self.fc3_mean = nn.Linear(30, 1)\n",
        "        # predicting varience\n",
        "        self.fc3_var = nn.Linear(30, 1)\n",
        "\n",
        "    # activations\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        mean = self.fc3_mean(x)\n",
        "        var = F.softplus(self.fc3_var(x)) + 1e-6\n",
        "        return mean, var\n",
        "\n",
        "# Using negative log-likihood for loss function\n",
        "def combined_loss(mean, var, target):\n",
        "    diff = target - mean\n",
        "    aleatoric_loss = (diff ** 2) / (2 * var) + 0.5 * torch.log(var)\n",
        "    return aleatoric_loss.mean()"
      ],
      "metadata": {
        "id": "vp7dxTJOerN3"
      },
      "id": "vp7dxTJOerN3",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    mean, var = model(X_train)\n",
        "    loss = combined_loss(mean, var, y_train)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "model.eval()\n",
        "T = 100  # forward passes\n",
        "means = torch.zeros((T, X_test.shape[0]))\n",
        "for t in range(T):\n",
        "    with torch.no_grad():\n",
        "        mean, _ = model(X_test)\n",
        "        means[t] = mean.view(-1)\n",
        "\n",
        "# Epistemic uncertainty is the variance of the means\n",
        "epistemic_var = means.var(dim=0).view(-1, 1).detach().numpy()\n",
        "mean_predictions = means.mean(dim=0).view(-1, 1).detach().numpy()\n",
        "\n",
        "# Aleatoric uncertainty\n",
        "with torch.no_grad():\n",
        "    _, aleatoric_var = model(X_test)\n",
        "    aleatoric_var = aleatoric_var.detach().numpy()\n",
        "\n",
        "# Total uncertainty\n",
        "total_uncertainty = aleatoric_var + epistemic_var\n",
        "\n",
        "print(f'Aleatoric Variance: {aleatoric_var}')\n",
        "print(f'Epistemic Variance: {epistemic_var}')\n",
        "print(f'Total Uncertainty: {total_uncertainty}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NVUNEUOqLW3",
        "outputId": "dfc3c0e7-ec3b-4cca-d4ea-9b23da1d0c9a"
      },
      "id": "4NVUNEUOqLW3",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 12.644700050354004\n",
            "Epoch 100, Loss: -0.9518958926200867\n",
            "Epoch 200, Loss: -1.101547122001648\n",
            "Epoch 300, Loss: -1.1859866380691528\n",
            "Epoch 400, Loss: -1.2476469278335571\n",
            "Epoch 500, Loss: -1.284010410308838\n",
            "Epoch 600, Loss: -1.1656535863876343\n",
            "Epoch 700, Loss: -1.1579521894454956\n",
            "Epoch 800, Loss: -1.2624183893203735\n",
            "Epoch 900, Loss: -1.3485400676727295\n",
            "Aleatoric Variance: [[0.06336854]\n",
            " [0.00188888]\n",
            " [0.22103576]\n",
            " [0.11977038]\n",
            " [0.01431656]\n",
            " [0.01779703]\n",
            " [0.0904642 ]\n",
            " [0.01877778]\n",
            " [0.0075925 ]\n",
            " [0.1103118 ]\n",
            " [0.03355786]\n",
            " [0.01259187]\n",
            " [0.00578335]\n",
            " [0.01378598]\n",
            " [0.03127936]\n",
            " [0.04783737]\n",
            " [0.0387359 ]\n",
            " [0.04904792]\n",
            " [0.10084379]\n",
            " [0.05922313]\n",
            " [0.01666602]\n",
            " [0.07752224]\n",
            " [0.02870195]\n",
            " [0.06414597]\n",
            " [0.00756699]\n",
            " [0.02798415]\n",
            " [0.02395366]\n",
            " [0.0177946 ]\n",
            " [0.01109564]\n",
            " [0.01690857]]\n",
            "Epistemic Variance: [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Total Uncertainty: [[0.06336854]\n",
            " [0.00188888]\n",
            " [0.22103576]\n",
            " [0.11977038]\n",
            " [0.01431656]\n",
            " [0.01779703]\n",
            " [0.0904642 ]\n",
            " [0.01877778]\n",
            " [0.0075925 ]\n",
            " [0.1103118 ]\n",
            " [0.03355786]\n",
            " [0.01259187]\n",
            " [0.00578335]\n",
            " [0.01378598]\n",
            " [0.03127936]\n",
            " [0.04783737]\n",
            " [0.0387359 ]\n",
            " [0.04904792]\n",
            " [0.10084379]\n",
            " [0.05922313]\n",
            " [0.01666602]\n",
            " [0.07752224]\n",
            " [0.02870195]\n",
            " [0.06414597]\n",
            " [0.00756699]\n",
            " [0.02798415]\n",
            " [0.02395366]\n",
            " [0.0177946 ]\n",
            " [0.01109564]\n",
            " [0.01690857]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}